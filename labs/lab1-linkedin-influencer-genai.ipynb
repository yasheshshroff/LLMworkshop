{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33ce6b46",
   "metadata": {
    "papermill": {
     "duration": 0.010207,
     "end_time": "2023-11-10T03:16:23.408677",
     "exception": false,
     "start_time": "2023-11-10T03:16:23.398470",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **LAB 1: DATA PREPARATION**\n",
    "\n",
    "In this notebook, we will explore how do prepare data for downstream NLP tasks.  \n",
    "\n",
    "A non-exhaustive list tasks related to NLP and Generative AI that can benefit from this data\n",
    "preparation is:\n",
    "\n",
    "- Question & Answer\n",
    "- Text Summarization\n",
    "- Instruct Tuning\n",
    "- Human-Bot Conversations\n",
    "- Continued PreTraining\n",
    "\n",
    "\n",
    "For each of these tasks, there are subtle differences in how one might want to prepare the data.\n",
    "Some of the key steps one might use here for data prep are also applicable to \n",
    "applying guardrails to LLMs (e.g., `Profanity Check` and `Toxicity Detection`)\n",
    "\n",
    "More on guardrails in Lab #5, but for now let's import our required python \n",
    "libraries and get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f976843",
   "metadata": {
    "papermill": {
     "duration": 0.00715,
     "end_time": "2023-11-10T03:16:23.423640",
     "exception": false,
     "start_time": "2023-11-10T03:16:23.416490",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 1: Persistent Python environment\n",
    "\n",
    "[Implementation](https://www.kaggle.com/code/kononenko/pip-install-once)\n",
    "[pip-install-forever](https://www.kaggle.com/code/samsammurphy/pip-install-forever)\n",
    "\n",
    "- Adjust the session persistence settings. \n",
    "  - Go to Notebook options (right pane) and set PERSISTENCE to Files only. You can also set it to Variables and Files, if you need persistent variables for your own reasons.\n",
    "  - Install in target directory: `pip install -r requirements.txt --target=/kaggle/working/workshop`\n",
    "  - Next:\n",
    "    1. Import sys: `import sys`\n",
    "    2. Add path: `sys.path.append(\"/kaggle/working/workshop\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c52b11-197a-4a09-8f72-e8f20ecc946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674f8e8c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 1.562386,
     "end_time": "2023-11-10T03:16:24.993330",
     "exception": false,
     "start_time": "2023-11-10T03:16:23.430944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting absl-py==2.0.0\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 130 kB 992 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting accelerate==0.24.0\n",
      "  Downloading accelerate-0.24.0-py3-none-any.whl (260 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 260 kB 870 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiobotocore==2.7.0\n",
      "  Downloading aiobotocore-2.7.0-py3-none-any.whl (73 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73 kB 910 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting aiohttp==3.8.5\n",
      "  Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.0 MB 1.1 MB/s eta 0:00:01     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 491 kB 400 kB/s eta 0:00:02\n",
      "\u001b[?25hCollecting aioitertools==0.11.0\n",
      "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: aiosignal==1.3.1 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r requirements_lab1.txt (line 6)) (1.3.1)\n",
      "Collecting alembic==1.12.1\n",
      "  Downloading alembic-1.12.1-py3-none-any.whl (226 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 226 kB 471 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting alt-profanity-check==1.3.1\n",
      "  Downloading alt-profanity-check-1.3.1.tar.gz (1.9 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.9 MB 819 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting annoy==1.17.3\n",
      "  Downloading annoy-1.17.3.tar.gz (647 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 647 kB 526 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting anyio==4.0.0\n",
      "  Using cached anyio-4.0.0-py3-none-any.whl (83 kB)\n",
      "Collecting async-timeout==4.0.3\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting attrs==23.1.0\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting beautifulsoup4==4.12.2\n",
      "  Using cached beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "Collecting blessed==1.20.0\n",
      "  Using cached blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
      "Collecting blinker==1.7.0\n",
      "  Downloading blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting botocore==1.31.64\n",
      "  Using cached botocore-1.31.64-py3-none-any.whl (11.3 MB)\n",
      "Collecting bs4==0.0.1\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Collecting cachetools==5.3.2\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Collecting certifi==2023.7.22\n",
      "  Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 158 kB 514 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer==3.3.1\n",
      "  Downloading charset_normalizer-3.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139 kB 509 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click==8.1.7\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97 kB 664 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting cloudpickle==2.2.1\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting colorama==0.4.6\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting contourpy==1.1.1\n",
      "  Downloading contourpy-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 301 kB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler==0.12.1\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting databricks-cli==0.18.0\n",
      "  Downloading databricks_cli-0.18.0-py2.py3-none-any.whl (150 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150 kB 686 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses-json==0.5.14\n",
      "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
      "Collecting datasets==2.14.6\n",
      "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 493 kB 736 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill==0.3.7\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docker==6.1.3\n",
      "  Downloading docker-6.1.3-py3-none-any.whl (148 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 148 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: entrypoints==0.4 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r requirements_lab1.txt (line 31)) (0.4)\n",
      "Collecting evaluate==0.4.1\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 84 kB 932 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting fastapi==0.96.0\n",
      "  Downloading fastapi-0.96.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 57 kB 367 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock==3.12.4\n",
      "  Using cached filelock-3.12.4-py3-none-any.whl (11 kB)\n",
      "Collecting Flask==3.0.0\n",
      "  Downloading flask-3.0.0-py3-none-any.whl (99 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99 kB 475 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fonttools==4.43.1\n",
      "  Downloading fonttools-4.43.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |‚ñã                               | 92 kB 600 kB/s eta 0:00:08^C\n",
      "\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements_lab1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b28b8482",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-11-10T03:16:25.010112Z",
     "iopub.status.busy": "2023-11-10T03:16:25.009847Z",
     "iopub.status.idle": "2023-11-10T03:16:43.866693Z",
     "shell.execute_reply": "2023-11-10T03:16:43.865977Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 18.867508,
     "end_time": "2023-11-10T03:16:43.868885",
     "exception": false,
     "start_time": "2023-11-10T03:16:25.001377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting profanity_check\n",
      "  Downloading profanity_check-1.0.3-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn>=0.20.2 (from profanity_check)\n",
      "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy<2.0,>=1.17.3 (from scikit-learn>=0.20.2->profanity_check)\n",
      "  Downloading numpy-1.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.5.0 (from scikit-learn>=0.20.2->profanity_check)\n",
      "  Downloading scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.1.1 (from scikit-learn>=0.20.2->profanity_check)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0 (from scikit-learn>=0.20.2->profanity_check)\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn, profanity_check\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.1 which is incompatible.\n",
      "momepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "numba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 1.26.1 which is incompatible.\n",
      "pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.26.1 which is incompatible.\n",
      "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.3 which is incompatible.\n",
      "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.26.1 which is incompatible.\n",
      "woodwork 0.26.0 requires numpy<1.25.0,>=1.22.0, but you have numpy 1.26.1 which is incompatible.\n",
      "ydata-profiling 4.3.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.26.1 which is incompatible.\n",
      "ydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed joblib-1.3.2 numpy-1.26.1 profanity_check-1.0.3 scikit-learn-1.3.2 scipy-1.11.3 threadpoolctl-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install profanity_check --target=/kaggle/working/workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c01b1b57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T03:16:43.888857Z",
     "iopub.status.busy": "2023-11-10T03:16:43.888557Z",
     "iopub.status.idle": "2023-11-10T03:16:43.892394Z",
     "shell.execute_reply": "2023-11-10T03:16:43.891635Z"
    },
    "papermill": {
     "duration": 0.016007,
     "end_time": "2023-11-10T03:16:43.894304",
     "exception": false,
     "start_time": "2023-11-10T03:16:43.878297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/kaggle/working/workshop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eae35b",
   "metadata": {
    "papermill": {
     "duration": 0.008621,
     "end_time": "2023-11-10T03:16:43.912458",
     "exception": false,
     "start_time": "2023-11-10T03:16:43.903837",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Required Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d693d8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T03:16:43.931459Z",
     "iopub.status.busy": "2023-11-10T03:16:43.931185Z",
     "iopub.status.idle": "2023-11-10T03:16:56.788686Z",
     "shell.execute_reply": "2023-11-10T03:16:56.787466Z"
    },
    "papermill": {
     "duration": 12.868505,
     "end_time": "2023-11-10T03:16:56.789952",
     "exception": true,
     "start_time": "2023-11-10T03:16:43.921447",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'joblib' from 'sklearn.externals' (/opt/conda/lib/python3.10/site-packages/sklearn/externals/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprofanity_check\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m predict_prob\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n",
      "File \u001b[0;32m/kaggle/working/workshop/profanity_check/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprofanity_check\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m predict, predict_prob\n\u001b[1;32m      2\u001b[0m __version__\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/kaggle/working/workshop/profanity_check/profanity_check.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpkg_resources\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m joblib\n\u001b[1;32m      5\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(pkg_resources\u001b[38;5;241m.\u001b[39mresource_filename(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprofanity_check\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/vectorizer.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(pkg_resources\u001b[38;5;241m.\u001b[39mresource_filename(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprofanity_check\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/model.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'joblib' from 'sklearn.externals' (/opt/conda/lib/python3.10/site-packages/sklearn/externals/__init__.py)"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set flag for training environment\n",
    "TRAINING = True\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from profanity_check import predict_prob\n",
    "\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10002bc4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Dataset\n",
    "\n",
    "Training dataset: Text content and metadata for LinkedIn posts during 2021. These posts were collected from the\n",
    "internet and correspond to a wide array of identified \"influencers\".\n",
    "\n",
    "Source: https://www.kaggle.com/datasets/shreyasajal/linkedin-influencers-data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddce8f2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Import Dataset\n",
    "\n",
    "To the right of this page, go to \"Add Data\" and search for \"LinkedIn Influencers' Data\". Hit the \"+\" icon to add the data to your `/kaggle/input` data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadbc183",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:00.121461Z",
     "iopub.status.busy": "2023-11-10T01:50:00.120562Z",
     "iopub.status.idle": "2023-11-10T01:50:03.834591Z",
     "shell.execute_reply": "2023-11-10T01:50:03.831388Z",
     "shell.execute_reply.started": "2023-11-10T01:50:00.121405Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in from public S3 Location\n",
    "df = pd.read_csv(\"/kaggle/input/linkedin-influencers-data/influencers_data.csv\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc32b911",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:04.892358Z",
     "iopub.status.busy": "2023-11-10T01:50:04.891751Z",
     "iopub.status.idle": "2023-11-10T01:50:04.997907Z",
     "shell.execute_reply": "2023-11-10T01:50:04.995978Z",
     "shell.execute_reply.started": "2023-11-10T01:50:04.892318Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26776941",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:05.830658Z",
     "iopub.status.busy": "2023-11-10T01:50:05.828425Z",
     "iopub.status.idle": "2023-11-10T01:50:05.843490Z",
     "shell.execute_reply": "2023-11-10T01:50:05.841077Z",
     "shell.execute_reply.started": "2023-11-10T01:50:05.830586Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbda2b6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "There are approximately ~70 LinkedIn influencers in this dataset. Let's take a look at\n",
    "how many posts we have data on for each influencer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa9da0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:06.952167Z",
     "iopub.status.busy": "2023-11-10T01:50:06.951656Z",
     "iopub.status.idle": "2023-11-10T01:50:08.203141Z",
     "shell.execute_reply": "2023-11-10T01:50:08.201212Z",
     "shell.execute_reply.started": "2023-11-10T01:50:06.952131Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# count how many records we have for influencers\n",
    "df.name.value_counts().sort_values().plot(kind='barh', figsize=(8,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8051f3f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Create Small Sample DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34014e18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:09.248160Z",
     "iopub.status.busy": "2023-11-10T01:50:09.247639Z",
     "iopub.status.idle": "2023-11-10T01:50:09.275727Z",
     "shell.execute_reply": "2023-11-10T01:50:09.274577Z",
     "shell.execute_reply.started": "2023-11-10T01:50:09.248122Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df = df.sample(100)[['name', 'headline', 'content']]\n",
    "\n",
    "# drop records with missing content\n",
    "sample_df = sample_df.dropna()\n",
    "\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c476dbbd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Custom Functions\n",
    "\n",
    "Set up a custom function to help with the data transformation. \n",
    "\n",
    "```\n",
    "def custom_function(df, text_columns):\n",
    "    \n",
    "    # For text cleaning based functions use: \n",
    "    for column in text_columns: \n",
    "        df[column] = # add change code here \n",
    "    \n",
    "    #For an example of how to filter the df see number_proportion_filter.py or numeric_filer.py \n",
    "    \n",
    "    return df\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897a82e5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# üôà Profanity Check\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f52648",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:11.258071Z",
     "iopub.status.busy": "2023-11-10T01:50:11.257532Z",
     "iopub.status.idle": "2023-11-10T01:50:11.307660Z",
     "shell.execute_reply": "2023-11-10T01:50:11.306721Z",
     "shell.execute_reply.started": "2023-11-10T01:50:11.258025Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from profanity_check import predict_prob\n",
    "\n",
    "threshold = 0.9\n",
    "\n",
    "sample_df[\"profanity\"] = predict_prob(sample_df[\"content\"])\n",
    "\n",
    "sample_df = sample_df[sample_df[\"profanity\"] < threshold]\n",
    "sample_df = sample_df.reset_index(drop=True)\n",
    "sample_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653368f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:11.945721Z",
     "iopub.status.busy": "2023-11-10T01:50:11.944630Z",
     "iopub.status.idle": "2023-11-10T01:50:12.298906Z",
     "shell.execute_reply": "2023-11-10T01:50:12.297561Z",
     "shell.execute_reply.started": "2023-11-10T01:50:11.945672Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df['profanity'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e7e842",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# üßê Text Quality Check\n",
    "\n",
    "## Flesch Grade reading level\n",
    "\n",
    "Depending on the task at hand, sometimes it's advantages to evaluate text and\n",
    "language based on it's estimated reading level. One popular way to do this \n",
    "is with the Flesch-Kincaid grade level.  This is a grade formula in that a score\n",
    "of 9.3 means that a ninth grader would be able to read the document.\n",
    "\n",
    "The wikipedia article on this technique does a fair job explaining it:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b1374f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Using the `textstat` python linvrary, we can approximate the reading level and\n",
    "set our thresholds for what level we find ideal for our downstream NLP task.\n",
    "\n",
    "In this case, we will limit the reading level between `2` and `10`, but many\n",
    "other ranges are certainly possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2587d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:14.284426Z",
     "iopub.status.busy": "2023-11-10T01:50:14.283577Z",
     "iopub.status.idle": "2023-11-10T01:50:14.562179Z",
     "shell.execute_reply": "2023-11-10T01:50:14.560781Z",
     "shell.execute_reply.started": "2023-11-10T01:50:14.284382Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import textstat \n",
    "\n",
    "min_grade_level = 2\n",
    "max_grade_level = 10\n",
    "\n",
    "def check_quality(df, text_column):\n",
    "    df[\"flesch_grade\"] = df[text_column].apply(textstat.flesch_kincaid_grade)\n",
    "    return df\n",
    "\n",
    "cleaned = check_quality(sample_df, 'content')\n",
    "\n",
    "# Filter to only those records with content between 2 and 10 grade level\n",
    "cleaned = cleaned[(cleaned[\"flesch_grade\"] >= min_grade_level) & (cleaned[\"flesch_grade\"] <= max_grade_level)]\n",
    "cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279d4b96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:15.487127Z",
     "iopub.status.busy": "2023-11-10T01:50:15.486591Z",
     "iopub.status.idle": "2023-11-10T01:50:15.816502Z",
     "shell.execute_reply": "2023-11-10T01:50:15.815114Z",
     "shell.execute_reply.started": "2023-11-10T01:50:15.487088Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df['flesch_grade'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dc3a21",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Run Preparation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebec03f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:17.353282Z",
     "iopub.status.busy": "2023-11-10T01:50:17.352386Z",
     "iopub.status.idle": "2023-11-10T01:50:32.076379Z",
     "shell.execute_reply": "2023-11-10T01:50:32.075075Z",
     "shell.execute_reply.started": "2023-11-10T01:50:17.353237Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic ETL\n",
    "\n",
    "# Read in some additional utility functions\n",
    "\n",
    "def remove_trailing_ws(df, text_columns):\n",
    "    # For text cleaning based functions use: \n",
    "    for column in text_columns: \n",
    "        df[column] = df[column].str.strip()\n",
    "    return df\n",
    "\n",
    "def length_check_func(df,text_column, minLength,maxLength):\n",
    "    df = df[(df[text_column].str.len() > int(minLength))]\n",
    "    df = df[(df[text_column].str.len() < int(maxLength))]\n",
    "    return df\n",
    "\n",
    "def string_replace(df, text_columns, input, output):\n",
    "    # For text cleaning based functions use: \n",
    "    for column in text_columns: \n",
    "        df[column] = df[column].str.replace(input, output)\n",
    "    return df\n",
    "\n",
    "# Keep Relevant Columns\n",
    "cleaned = df[['name', 'headline', 'about', 'content', 'reactions']]\n",
    "\n",
    "# Dropping missing\n",
    "cleaned = cleaned.dropna()\n",
    "\n",
    "\n",
    "# Text Cleaning (Minimal)\n",
    "cleaned = string_replace(cleaned, ['content'], '‚Ä¶see more', '')\n",
    "cleaned = remove_trailing_ws(cleaned, ['content'])\n",
    "\n",
    "max_len = 10000\n",
    "min_len = 1\n",
    "\n",
    "cleaned = length_check_func(cleaned, 'content', min_len, max_len)\n",
    "\n",
    "\n",
    "# Set thresholds\n",
    "\n",
    "# Profanity Threshold\n",
    "profanity_threshold = 0.9\n",
    "\n",
    "# Target Reading Level\n",
    "min_grade_level = 2\n",
    "max_grade_level = 10\n",
    "\n",
    "# Predict Probability of Profane Language\n",
    "cleaned[\"profanity\"] = predict_prob(cleaned[\"content\"])\n",
    "\n",
    "# Filter out Profane Language\n",
    "cleaned = cleaned[cleaned[\"profanity\"] < profanity_threshold]\n",
    "cleaned = cleaned.reset_index(drop=True)\n",
    "\n",
    "# Determine Reading Level\n",
    "cleaned = check_quality(cleaned, 'content')\n",
    "\n",
    "# Filter to only those records with content between a 2nd and 10th grader\n",
    "cleaned = cleaned[(cleaned[\"flesch_grade\"] >= min_grade_level) & (cleaned[\"flesch_grade\"] <= max_grade_level)]\n",
    "cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dedfa86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:32.079081Z",
     "iopub.status.busy": "2023-11-10T01:50:32.078554Z",
     "iopub.status.idle": "2023-11-10T01:50:32.085937Z",
     "shell.execute_reply": "2023-11-10T01:50:32.084738Z",
     "shell.execute_reply.started": "2023-11-10T01:50:32.079048Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_dropped = df.shape[0] - cleaned.shape[0]\n",
    "\n",
    "print(f\"Lost a total of {num_dropped} records, or about {round(num_dropped/df.shape[0], 2) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82311a97",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# One Step Further\n",
    "\n",
    "If we really want to talk like an influencer, perhaps we should also additionally limit the number of records\n",
    "by the number of reactions the posts got. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb86e51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:34.300043Z",
     "iopub.status.busy": "2023-11-10T01:50:34.299545Z",
     "iopub.status.idle": "2023-11-10T01:50:34.326470Z",
     "shell.execute_reply": "2023-11-10T01:50:34.325042Z",
     "shell.execute_reply.started": "2023-11-10T01:50:34.300006Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned.sort_values('reactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205e140d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We can see that the number of reactions these posts from 'influencers' received appear to go \n",
    "from zero (yikes!) all the way to over 330k. Noticeably, that single post with ~330k reactions\n",
    "is simply `Helen is my kinda lady`.\n",
    "\n",
    "The fact this post received so much attention given it's lack of context is also \n",
    "a warning that there could be exogenous latent variables, such as current events, pop culture, etc, that could be driving \n",
    "the number of reactions, not necessarily the content itself. \n",
    "\n",
    "Let's be just a little more analytical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd069513",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:39.650715Z",
     "iopub.status.busy": "2023-11-10T01:50:39.649090Z",
     "iopub.status.idle": "2023-11-10T01:50:39.661174Z",
     "shell.execute_reply": "2023-11-10T01:50:39.659715Z",
     "shell.execute_reply.started": "2023-11-10T01:50:39.650652Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify the 90th percentile of reactions over all posts\n",
    "p90 = np.quantile(cleaned.reactions, 0.9)\n",
    "\n",
    "print(f'The 90th percentile is {p90} reactions.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcfdc74",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "For our purposes, let's focus on the top \"performing\" content to fine tune our model on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11752a02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:41.078148Z",
     "iopub.status.busy": "2023-11-10T01:50:41.077488Z",
     "iopub.status.idle": "2023-11-10T01:50:41.090022Z",
     "shell.execute_reply": "2023-11-10T01:50:41.088789Z",
     "shell.execute_reply.started": "2023-11-10T01:50:41.078098Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned = cleaned[cleaned.reactions > p90]\n",
    "cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f711c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We're now left with 2,127 high-quality data points to experiment with \n",
    "fine tuning on. \n",
    "\n",
    "Finally, let's ask h2oGPT to provide a title for our LinkedIn Influencer content.\n",
    "This process is called `zero-shot text generation` (more on this later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b8475",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:51:12.369670Z",
     "iopub.status.busy": "2023-11-10T01:51:12.369182Z",
     "iopub.status.idle": "2023-11-10T01:51:21.610939Z",
     "shell.execute_reply": "2023-11-10T01:51:21.609347Z",
     "shell.execute_reply.started": "2023-11-10T01:51:12.369634Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df = cleaned.sample(5)\n",
    "\n",
    "from gradio_client import Client\n",
    "import ast\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "HOST_URL = \"https://gpt-genai.h2o.ai/\"\n",
    "GPT_KEY = \"f74f043e-45fc-4dfe-9c33-55a4720427f6\"\n",
    "    \n",
    "client = Client(HOST_URL)\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def generate_title(content):\n",
    "    \n",
    "    #try:\n",
    "    summarize_prompt = 'You are a helpful, respectful and honest assistant the specializes in generating accurate titles of LinkedIn posts. Provide a title for the following post. The title should be a single sentence, not using bullet points. Only include the title in the response. The LinkedIn post is: ' + content\n",
    "    kwargs = dict(\n",
    "        instruction_nochat=summarize_prompt, \n",
    "        h2ogpt_key = GPT_KEY)\n",
    "    \n",
    "    response = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')\n",
    "    reply = ast.literal_eval(response)['response']\n",
    "    #except:\n",
    "        #reply = 'NA'\n",
    "    return reply\n",
    "        \n",
    "sample_df['title'] = sample_df.progress_apply(lambda row :   generate_title(row['content']), axis=1)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8040bb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:51:22.779132Z",
     "iopub.status.busy": "2023-11-10T01:51:22.778601Z",
     "iopub.status.idle": "2023-11-10T01:51:22.788148Z",
     "shell.execute_reply": "2023-11-10T01:51:22.786648Z",
     "shell.execute_reply.started": "2023-11-10T01:51:22.779093Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pprint(f'CONTENT:')\n",
    "sample_df['content'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001acb4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:51:23.762650Z",
     "iopub.status.busy": "2023-11-10T01:51:23.761810Z",
     "iopub.status.idle": "2023-11-10T01:51:23.770735Z",
     "shell.execute_reply": "2023-11-10T01:51:23.769707Z",
     "shell.execute_reply.started": "2023-11-10T01:51:23.762609Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'TITLE:')\n",
    "sample_df['title'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf5c7ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:51:24.548964Z",
     "iopub.status.busy": "2023-11-10T01:51:24.547836Z",
     "iopub.status.idle": "2023-11-10T01:51:24.574766Z",
     "shell.execute_reply": "2023-11-10T01:51:24.573266Z",
     "shell.execute_reply.started": "2023-11-10T01:51:24.548917Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional alternative for fine tuning - create an instruction.\n",
    "cleaned['instruction'] = ('Write a LinkedIn post in the style of an influencer whom has the title of '\n",
    "  + cleaned['headline'] + ' and can be described by the following: ' \n",
    "  + cleaned['about'])\n",
    "\n",
    "cleaned.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae17bc7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Output Dataset\n",
    "\n",
    "Now we're ready to store our data set out and experiment with fine-tuning\n",
    "in Lab # 2 with H2O LLM Studio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b25ad",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-11-10T01:52:01.078715Z",
     "iopub.status.busy": "2023-11-10T01:52:01.078185Z",
     "iopub.status.idle": "2023-11-10T03:01:13.098104Z",
     "shell.execute_reply": "2023-11-10T03:01:13.096087Z",
     "shell.execute_reply.started": "2023-11-10T01:52:01.078668Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# WARNING! This could take a very long time using the public facing h2oGPT endpoint. ##\n",
    "#######################################################################################\n",
    "\n",
    "# Apply to full data set\n",
    "cleaned['title'] = cleaned.progress_apply(lambda row :   generate_title(row['content']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c630f9f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T03:12:13.080002Z",
     "iopub.status.busy": "2023-11-10T03:12:13.079498Z",
     "iopub.status.idle": "2023-11-10T03:12:13.513635Z",
     "shell.execute_reply": "2023-11-10T03:12:13.512360Z",
     "shell.execute_reply.started": "2023-11-10T03:12:13.079963Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Output locally\n",
    "cleaned.to_csv('/kaggle/working/influencers_data_prepared.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc2bdd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d814d2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Understanding Data Prep for Production\n",
    "\n",
    "In this lab, we cleaned the data and fine-tuned with with Title column added from the Content field. \n",
    "\n",
    "Other pipeline components include:\n",
    "\n",
    "- Data Augmentation\n",
    "- Text Cleaning\n",
    "- Profanity Check \n",
    "- Text Quality Check\n",
    "- Length Checker \n",
    "- Valid Question\n",
    "- Pad Sequence\n",
    "- Truncate Sequence by Score \n",
    "- Compression Ratio filter\n",
    "- Boundary Marking\n",
    "- Sensitive Info Checker\n",
    "- RLHF Protection\n",
    "- Language Understanding \n",
    "- Data Deduplication\n",
    "- Toxicity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebe4062",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 38.069262,
   "end_time": "2023-11-10T03:16:58.522503",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-10T03:16:20.453241",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
