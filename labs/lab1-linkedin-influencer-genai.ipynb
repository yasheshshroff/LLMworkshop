{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33ce6b46",
   "metadata": {
    "papermill": {
     "duration": 0.010207,
     "end_time": "2023-11-10T03:16:23.408677",
     "exception": false,
     "start_time": "2023-11-10T03:16:23.398470",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **LAB 1: DATA PREPARATION**\n",
    "\n",
    "In this notebook, we will explore how do prepare data for downstream NLP tasks.  \n",
    "\n",
    "A non-exhaustive list tasks related to NLP and Generative AI that can benefit from this data\n",
    "preparation is:\n",
    "\n",
    "- Question & Answer\n",
    "- Text Summarization\n",
    "- Instruct Tuning\n",
    "- Human-Bot Conversations\n",
    "- Continued PreTraining\n",
    "\n",
    "\n",
    "For each of these tasks, there are subtle differences in how one might want to prepare the data.\n",
    "Some of the key steps one might use here for data prep are also applicable to \n",
    "applying guardrails to LLMs (e.g., `Profanity Check` and `Toxicity Detection`)\n",
    "\n",
    "More on guardrails in Lab #5, but for now let's import our required python \n",
    "libraries and get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f976843",
   "metadata": {
    "papermill": {
     "duration": 0.00715,
     "end_time": "2023-11-10T03:16:23.423640",
     "exception": false,
     "start_time": "2023-11-10T03:16:23.416490",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 1: Persistent Python environment\n",
    "\n",
    "[Implementation](https://www.kaggle.com/code/kononenko/pip-install-once)\n",
    "[pip-install-forever](https://www.kaggle.com/code/samsammurphy/pip-install-forever)\n",
    "\n",
    "- Adjust the session persistence settings. \n",
    "  - Go to Notebook options (right pane) and set PERSISTENCE to Files only. You can also set it to Variables and Files, if you need persistent variables for your own reasons.\n",
    "  - Install in target directory: `pip install -r requirements.txt --target=/kaggle/working/workshop`\n",
    "  - Next:\n",
    "    1. Import sys: `import sys`\n",
    "    2. Add path: `sys.path.append(\"/kaggle/working/workshop\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f8e8c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 1.562386,
     "end_time": "2023-11-10T03:16:24.993330",
     "exception": false,
     "start_time": "2023-11-10T03:16:23.430944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting absl-py==2.0.0\n",
      "  Using cached absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "Collecting accelerate==0.24.0\n",
      "  Using cached accelerate-0.24.0-py3-none-any.whl (260 kB)\n",
      "Collecting aiobotocore==2.7.0\n",
      "  Using cached aiobotocore-2.7.0-py3-none-any.whl (73 kB)\n",
      "Collecting aiohttp==3.8.5\n",
      "  Using cached aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Collecting aioitertools==0.11.0\n",
      "  Using cached aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: aiosignal==1.3.1 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 6)) (1.3.1)\n",
      "Collecting alembic==1.12.1\n",
      "  Using cached alembic-1.12.1-py3-none-any.whl (226 kB)\n",
      "Collecting alt-profanity-check==1.3.1\n",
      "  Using cached alt-profanity-check-1.3.1.tar.gz (1.9 MB)\n",
      "Collecting annoy==1.17.3\n",
      "  Using cached annoy-1.17.3.tar.gz (647 kB)\n",
      "Collecting anyio==4.0.0\n",
      "  Using cached anyio-4.0.0-py3-none-any.whl (83 kB)\n",
      "Collecting async-timeout==4.0.3\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting attrs==23.1.0\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting beautifulsoup4==4.12.2\n",
      "  Using cached beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "Collecting blessed==1.20.0\n",
      "  Using cached blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
      "Collecting blinker==1.7.0\n",
      "  Using cached blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting botocore==1.31.64\n",
      "  Using cached botocore-1.31.64-py3-none-any.whl (11.3 MB)\n",
      "Collecting bs4==0.0.1\n",
      "  Using cached bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Collecting cachetools==5.3.2\n",
      "  Using cached cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Collecting certifi==2023.7.22\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "Collecting charset-normalizer==3.3.1\n",
      "  Using cached charset_normalizer-3.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
      "Collecting click==8.1.7\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Collecting cloudpickle==2.2.1\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting colorama==0.4.6\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting contourpy==1.1.1\n",
      "  Using cached contourpy-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Collecting cycler==0.12.1\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting databricks-cli==0.18.0\n",
      "  Using cached databricks_cli-0.18.0-py2.py3-none-any.whl (150 kB)\n",
      "Collecting dataclasses-json==0.5.14\n",
      "  Using cached dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
      "Collecting datasets==2.14.6\n",
      "  Using cached datasets-2.14.6-py3-none-any.whl (493 kB)\n",
      "Collecting dill==0.3.7\n",
      "  Using cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Collecting docker==6.1.3\n",
      "  Using cached docker-6.1.3-py3-none-any.whl (148 kB)\n",
      "Requirement already satisfied: entrypoints==0.4 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 31)) (0.4)\n",
      "Collecting evaluate==0.4.1\n",
      "  Using cached evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "Collecting fastapi==0.96.0\n",
      "  Using cached fastapi-0.96.0-py3-none-any.whl (57 kB)\n",
      "Collecting filelock==3.12.4\n",
      "  Using cached filelock-3.12.4-py3-none-any.whl (11 kB)\n",
      "Collecting Flask==3.0.0\n",
      "  Using cached flask-3.0.0-py3-none-any.whl (99 kB)\n",
      "Collecting fonttools==4.43.1\n",
      "  Downloading fonttools-4.43.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 659 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting frozenlist==1.4.0\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "\u001b[K     |████████████████████████████████| 225 kB 758 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec==2023.10.0\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[K     |████████████████████████████████| 166 kB 952 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gitdb==4.0.11\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 480 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting GitPython==3.1.40\n",
      "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
      "\u001b[K     |████████████████████████████████| 190 kB 930 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth==2.23.3\n",
      "  Downloading google_auth-2.23.3-py2.py3-none-any.whl (182 kB)\n",
      "\u001b[K     |████████████████████████████████| 182 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib==1.1.0\n",
      "  Downloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting gradio_client==0.3.0\n",
      "  Downloading gradio_client-0.3.0-py3-none-any.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 461 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting greenlet==3.0.1\n",
      "  Downloading greenlet-3.0.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (613 kB)\n",
      "\u001b[K     |████████████████████████████████| 613 kB 841 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio==1.59.0\n",
      "  Downloading grpcio-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.3 MB 431 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gunicorn==21.2.0\n",
      "  Downloading gunicorn-21.2.0-py3-none-any.whl (80 kB)\n",
      "\u001b[K     |████████████████████████████████| 80 kB 997 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h11==0.14.0 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 47)) (0.14.0)\n",
      "Collecting h2o-authn==1.1.0\n",
      "  Downloading h2o_authn-1.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting h2o-mlops==0.62.1a1\n",
      "  Downloading h2o_mlops-0.62.1a1-py3-none-any.whl (771 kB)\n",
      "\u001b[K     |████████████████████████████████| 771 kB 608 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h2o-wave==1.0.0\n",
      "  Downloading h2o_wave-1.0.0-py3-none-manylinux1_x86_64.whl (12.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.1 MB 712 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h2ogpte==0.9.2\n",
      "  Downloading h2ogpte-0.9.2-py3-none-any.whl (7.8 kB)\n",
      "Collecting httpcore==0.16.3\n",
      "  Using cached httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
      "Collecting httpx==0.23.3\n",
      "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
      "\u001b[K     |████████████████████████████████| 71 kB 574 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting huggingface-hub==0.17.3\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[K     |████████████████████████████████| 295 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna==3.4\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 77 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting inquirer==3.1.3\n",
      "  Using cached inquirer-3.1.3-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: itsdangerous==2.1.2 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 57)) (2.1.2)\n",
      "Collecting Jinja2==3.1.2\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath==1.0.1\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting joblib==1.3.2\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[K     |████████████████████████████████| 302 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver==1.4.5\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 756 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting langchain==0.0.251\n",
      "  Downloading langchain-0.0.251-py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 308 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting langsmith==0.0.53\n",
      "  Downloading langsmith-0.0.53-py3-none-any.whl (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 813 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting llvmlite==0.41.1\n",
      "  Downloading llvmlite-0.41.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 43.6 MB 24 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting lxml==4.9.3\n",
      "  Downloading lxml-4.9.3-cp310-cp310-manylinux_2_28_x86_64.whl (7.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.9 MB 220 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Mako==1.2.4 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 66)) (1.2.4)\n",
      "Collecting Markdown==3.5\n",
      "  Downloading Markdown-3.5-py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 1.4 MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting MarkupSafe==2.1.3\n",
      "  Downloading MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting marshmallow==3.20.1\n",
      "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 770 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting matplotlib==3.8.0\n",
      "  Downloading matplotlib-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.6 MB 79 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting mlflow==2.8.0\n",
      "  Downloading mlflow-2.8.0-py3-none-any.whl (19.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.0 MB 35 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting mpmath==1.3.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Requirement already satisfied: multidict==6.0.4 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 73)) (6.0.4)\n",
      "Collecting multiprocess==0.70.15\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[K     |████████████████████████████████| 134 kB 480 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: mypy-extensions==1.0.0 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 75)) (1.0.0)\n",
      "Collecting nemo-toolkit==1.21.0\n",
      "  Downloading nemo_toolkit-1.21.0-py3-none-any.whl (2.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5 MB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nemoguardrails==0.5.0\n",
      "  Downloading nemoguardrails-0.5.0-py3-none-any.whl (13.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.9 MB 142 kB/s eta 0:00:01    |██▌                             | 1.1 MB 2.3 MB/s eta 0:00:06\n",
      "\u001b[?25hCollecting nest-asyncio==1.5.6\n",
      "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
      "Collecting networkx==3.2\n",
      "  Downloading networkx-3.2-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 981 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nltk==3.8.1 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 80)) (3.8.1)\n",
      "Collecting numba==0.58.1\n",
      "  Downloading numba-0.58.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numexpr==2.8.7\n",
      "  Downloading numexpr-2.8.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384 kB)\n",
      "\u001b[K     |████████████████████████████████| 384 kB 371 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy==1.23.5\n",
      "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.1 MB 77 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting oauthlib==3.2.2\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 163 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting onnx==1.15.0\n",
      "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7 MB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting openai==0.28.1\n",
      "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 829 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting openapi-schema-pydantic==1.2.4\n",
      "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 2.4 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pandas==2.1.1\n",
      "  Downloading pandas-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.3 MB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Pillow==10.1.0\n",
      "  Downloading Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 356 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting portalocker==2.8.2\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Collecting protobuf==4.23.4\n",
      "  Using cached protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "Collecting pyarrow==13.0.0\n",
      "  Downloading pyarrow-13.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 40.0 MB 3.9 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1==0.5.0\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 335 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules==0.3.0\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic==1.10.13\n",
      "  Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 175 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyJWT==2.8.0\n",
      "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Collecting pyparsing==3.1.1\n",
      "  Downloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyphen==0.14.0\n",
      "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-dotenv==1.0.0\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting python-editor==1.0.4\n",
      "  Using cached python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
      "Collecting pytz==2023.3.post1\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[K     |████████████████████████████████| 502 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyYAML==6.0.1\n",
      "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "\u001b[K     |████████████████████████████████| 705 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting querystring-parser==1.2.4\n",
      "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting readchar==4.0.5\n",
      "  Using cached readchar-4.0.5-py3-none-any.whl (8.5 kB)\n",
      "Collecting regex==2023.10.3\n",
      "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[K     |████████████████████████████████| 773 kB 995 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests==2.31.0\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 636 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: requests-oauthlib==1.3.1 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 107)) (1.3.1)\n",
      "Requirement already satisfied: responses==0.18.0 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 108)) (0.18.0)\n",
      "Requirement already satisfied: rfc3986==1.5.0 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 109)) (1.5.0)\n",
      "Collecting rouge-score==0.1.2\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "Collecting rsa==4.9\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting ruamel.yaml==0.18.3\n",
      "  Downloading ruamel.yaml-0.18.3-py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 475 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ruamel.yaml.clib==0.2.8\n",
      "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
      "\u001b[K     |████████████████████████████████| 526 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3fs==2023.10.0\n",
      "  Downloading s3fs-2023.10.0-py3-none-any.whl (28 kB)\n",
      "Collecting sacrebleu==2.3.1\n",
      "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
      "\u001b[K     |████████████████████████████████| 118 kB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting safetensors==0.4.0\n",
      "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 520 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn==1.3.1\n",
      "  Downloading scikit_learn-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.8 MB 461 kB/s eta 0:00:0179 kB/s eta 0:00:120:11\n",
      "\u001b[?25hCollecting scipy==1.11.3\n",
      "  Downloading scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 36.4 MB 39 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting sentence-transformers==2.2.2\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 489 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece==0.1.99\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 645 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting simpleeval==0.9.13\n",
      "  Downloading simpleeval-0.9.13-py2.py3-none-any.whl (15 kB)\n",
      "Collecting smmap==5.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: sniffio==1.3.0 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 123)) (1.3.0)\n",
      "Collecting soupsieve==2.5\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Collecting SQLAlchemy==2.0.22\n",
      "  Downloading SQLAlchemy-2.0.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sqlparse==0.4.4\n",
      "  Downloading sqlparse-0.4.4-py3-none-any.whl (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 113 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting starlette==0.27.0\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "\u001b[K     |████████████████████████████████| 66 kB 3.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting sympy==1.12\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting tabulate==0.9.0\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting tenacity==8.2.3\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Collecting tensorboard==2.15.0\n",
      "  Downloading tensorboard-2.15.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server==0.7.2\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting text-unidecode==1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting textstat==0.7.3\n",
      "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl==3.2.0\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Collecting tokenizers==0.14.1\n",
      "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 790 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch==2.1.0\n",
      "  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 670.2 MB 335 bytes/s  0:00:012   |█                               | 21.1 MB 1.0 MB/s eta 0:10:36     |███▌                            | 73.9 MB 1.4 MB/s eta 0:07:03     |█████████████████▎              | 362.2 MB 1.2 MB/s eta 0:04:19[K     |██████████████████▊             | 392.4 MB 732 kB/s eta 0:06:20��████████████████████▏          | 444.3 MB 1.4 MB/s eta 0:02:46     |███████████████████████▋        | 494.8 MB 359 kB/s eta 0:08:08█████▊        | 496.5 MB 387 kB/s eta 0:07:29��████████████████▎       | 507.5 MB 2.0 MB/s eta 0:01:20     |████████████████████████▊       | 517.3 MB 1.6 MB/s eta 0:01:35     |██████████████████████████▍     | 552.0 MB 846 kB/s eta 0:02:20 | 565.5 MB 1.3 MB/s eta 0:01:19��████████▋    | 578.1 MB 513 kB/s eta 0:03:00��███████████████████▉   | 604.9 MB 2.1 MB/s eta 0:00:319 MB 917 kB/s eta 0:00:26     |███████████████████████████████▏| 652.1 MB 1.0 MB/s eta 0:00:18\n",
      "\u001b[?25hCollecting torchvision==0.16.0\n",
      "  Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.9 MB 693 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm==4.66.1\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Collecting transformers==4.34.1\n",
      "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.7 MB 169 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typer==0.7.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting typing-inspect==0.9.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting typing_extensions==4.5.0\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting tzdata==2023.3\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Collecting urllib3==2.0.7\n",
      "  Downloading urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "\u001b[K     |████████████████████████████████| 124 kB 355 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting uvicorn==0.22.0\n",
      "  Using cached uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
      "Collecting websocket-client==1.6.4\n",
      "  Using cached websocket_client-1.6.4-py3-none-any.whl (57 kB)\n",
      "Collecting websockets==11.0.3\n",
      "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 509 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Werkzeug==3.0.1\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "\u001b[K     |████████████████████████████████| 226 kB 672 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wget==3.2\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "Collecting wrapt==1.15.0\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 311 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xxhash==3.4.1\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[K     |████████████████████████████████| 194 kB 482 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl==1.9.2\n",
      "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
      "\u001b[K     |████████████████████████████████| 268 kB 254 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting profanity_check\n",
      "  Downloading profanity_check-1.0.3-py3-none-any.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: psutil in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from accelerate==0.24.0->-r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 2)) (5.9.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from accelerate==0.24.0->-r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 2)) (23.0)\n",
      "Collecting exceptiongroup>=1.0.2\n",
      "  Using cached exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from blessed==1.20.0->-r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 14)) (1.16.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from blessed==1.20.0->-r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 14)) (0.2.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from botocore==1.31.64->-r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 16)) (2.8.2)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from datasets==2.14.6->-r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 28)) (2023.1.0)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from mlflow==2.8.0->-r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 71)) (4.11.3)\n",
      "Collecting setuptools>=65.5.1\n",
      "  Downloading setuptools-68.2.2-py3-none-any.whl (807 kB)\n",
      "\u001b[K     |████████████████████████████████| 807 kB 545 kB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading setuptools-65.5.1-py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-nccl-cu12==2.18.1\n",
      "  Using cached nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "Collecting triton==2.1.0\n",
      "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 89.2 MB 50 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12\n",
      "  Downloading nvidia_nvjitlink_cu12-12.3.52-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.5 MB 1.6 MB/s eta 0:00:01    |█████████▋                      | 6.2 MB 249 kB/s eta 0:00:58\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow==2.8.0->-r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 71)) (3.8.0)\n",
      "Building wheels for collected packages: alt-profanity-check, annoy, bs4, rouge-score, sentence-transformers, wget\n",
      "  Building wheel for alt-profanity-check (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for alt-profanity-check: filename=alt_profanity_check-1.3.1-py3-none-any.whl size=1866705 sha256=d70c783207740257b74c729e4784d6b6bf5b3a82372d98062244b7d1c4ac5483\n",
      "  Stored in directory: /home/yashroff/.cache/pip/wheels/fd/c7/5e/5579c7145bae270d6a7896320ab0575a897cc7d71e84772e26\n",
      "  Building wheel for annoy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for annoy: filename=annoy-1.17.3-cp310-cp310-linux_x86_64.whl size=79201 sha256=ae36244c0cd7f943d86166d7a45802c40680046737cfaa70bbf2c1a5ed488e54\n",
      "  Stored in directory: /home/yashroff/.cache/pip/wheels/64/8a/da/f714bcf46c5efdcfcac0559e63370c21abe961c48e3992465a\n",
      "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=9faa644105da38a3ce1a360af0bc37539059885f792febaeef47bc855265d156\n",
      "  Stored in directory: /home/yashroff/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=9caef3acd2dde71a30f14639d924d7b343284101de68473246385d049eeffcb1\n",
      "  Stored in directory: /home/yashroff/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125940 sha256=1b96174cc96e426a0d25837a5ff0be3ded00468a6c6efa2ccba85f224763feeb\n",
      "  Stored in directory: /home/yashroff/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=5aa424f97644056e3eccb61cf8a01d591955e9a473bbabe9555ea5bb98dfad3f\n",
      "  Stored in directory: /home/yashroff/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
      "Successfully built alt-profanity-check annoy bs4 rouge-score sentence-transformers wget\n",
      "Installing collected packages: urllib3, nvidia-nvjitlink-cu12, idna, charset-normalizer, certifi, typing-extensions, tqdm, requests, PyYAML, pyasn1, nvidia-cusparse-cu12, nvidia-cublas-cu12, mpmath, MarkupSafe, fsspec, frozenlist, filelock, exceptiongroup, yarl, triton, sympy, rsa, pyasn1-modules, oauthlib, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusolver-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, numpy, networkx, Jinja2, huggingface-hub, cachetools, attrs, async-timeout, anyio, tzdata, typing-inspect, torch, tokenizers, threadpoolctl, soupsieve, smmap, setuptools, scipy, safetensors, regex, pytz, pydantic, Pillow, marshmallow, joblib, jmespath, httpcore, greenlet, google-auth, dill, click, aiohttp, xxhash, wrapt, Werkzeug, websocket-client, transformers, torchvision, tensorboard-data-server, tenacity, tabulate, starlette, SQLAlchemy, sentencepiece, scikit-learn, ruamel.yaml.clib, readchar, python-editor, python-dotenv, pyparsing, PyJWT, pyarrow, protobuf, pandas, openapi-schema-pydantic, numexpr, multiprocess, Markdown, llvmlite, langsmith, kiwisolver, httpx, grpcio, google-auth-oauthlib, gitdb, fonttools, dataclasses-json, cycler, contourpy, botocore, blinker, blessed, beautifulsoup4, aioitertools, absl-py, wget, websockets, uvicorn, typer, text-unidecode, tensorboard, sqlparse, simpleeval, sentence-transformers, ruamel.yaml, querystring-parser, pyphen, portalocker, onnx, numba, nest-asyncio, matplotlib, lxml, langchain, inquirer, h2o-authn, gunicorn, GitPython, Flask, fastapi, docker, datasets, databricks-cli, colorama, cloudpickle, bs4, annoy, alembic, aiobotocore, textstat, sacrebleu, s3fs, rouge-score, profanity-check, openai, nemoguardrails, nemo-toolkit, mlflow, h2ogpte, h2o-wave, h2o-mlops, gradio-client, evaluate, alt-profanity-check, accelerate\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.9\n",
      "    Uninstalling urllib3-1.26.9:\n",
      "      Successfully uninstalled urllib3-1.26.9\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.3\n",
      "    Uninstalling idna-3.3:\n",
      "      Successfully uninstalled idna-3.3\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 2.0.4\n",
      "    Uninstalling charset-normalizer-2.0.4:\n",
      "      Successfully uninstalled charset-normalizer-2.0.4\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2022.12.7\n",
      "    Uninstalling certifi-2022.12.7:\n",
      "      Successfully uninstalled certifi-2022.12.7\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.7.0\n",
      "    Uninstalling typing-extensions-4.7.0:\n",
      "      Successfully uninstalled typing-extensions-4.7.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.64.0\n",
      "    Uninstalling tqdm-4.64.0:\n",
      "      Successfully uninstalled tqdm-4.64.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.0\n",
      "    Uninstalling requests-2.28.0:\n",
      "      Successfully uninstalled requests-2.28.0\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "\u001b[33m      WARNING: Cannot remove entries from nonexistent file /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages/easy-install.pth\u001b[0m\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: pyasn1\n",
      "    Found existing installation: pyasn1 0.4.8\n",
      "    Uninstalling pyasn1-0.4.8:\n",
      "      Successfully uninstalled pyasn1-0.4.8\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.1\n",
      "    Uninstalling MarkupSafe-2.1.1:\n",
      "      Successfully uninstalled MarkupSafe-2.1.1\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.1.0\n",
      "    Uninstalling fsspec-2023.1.0:\n",
      "      Successfully uninstalled fsspec-2023.1.0\n",
      "  Attempting uninstall: frozenlist\n",
      "    Found existing installation: frozenlist 1.3.3\n",
      "    Uninstalling frozenlist-1.3.3:\n",
      "      Successfully uninstalled frozenlist-1.3.3\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.6.0\n",
      "    Uninstalling filelock-3.6.0:\n",
      "      Successfully uninstalled filelock-3.6.0\n",
      "  Attempting uninstall: yarl\n",
      "    Found existing installation: yarl 1.8.2\n",
      "    Uninstalling yarl-1.8.2:\n",
      "      Successfully uninstalled yarl-1.8.2\n",
      "  Attempting uninstall: rsa\n",
      "    Found existing installation: rsa 4.8\n",
      "    Uninstalling rsa-4.8:\n",
      "      Successfully uninstalled rsa-4.8\n",
      "  Attempting uninstall: pyasn1-modules\n",
      "    Found existing installation: pyasn1-modules 0.2.8\n",
      "    Uninstalling pyasn1-modules-0.2.8:\n",
      "      Successfully uninstalled pyasn1-modules-0.2.8\n",
      "  Attempting uninstall: oauthlib\n",
      "    Found existing installation: oauthlib 3.2.0\n",
      "    Uninstalling oauthlib-3.2.0:\n",
      "      Successfully uninstalled oauthlib-3.2.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.3\n",
      "    Uninstalling numpy-1.22.3:\n",
      "      Successfully uninstalled numpy-1.22.3\n",
      "  Attempting uninstall: Jinja2\n",
      "    Found existing installation: Jinja2 3.0.3\n",
      "    Uninstalling Jinja2-3.0.3:\n",
      "      Successfully uninstalled Jinja2-3.0.3\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.12.0\n",
      "    Uninstalling huggingface-hub-0.12.0:\n",
      "      Successfully uninstalled huggingface-hub-0.12.0\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 5.2.0\n",
      "    Uninstalling cachetools-5.2.0:\n",
      "      Successfully uninstalled cachetools-5.2.0\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 21.4.0\n",
      "    Uninstalling attrs-21.4.0:\n",
      "      Successfully uninstalled attrs-21.4.0\n",
      "  Attempting uninstall: async-timeout\n",
      "    Found existing installation: async-timeout 4.0.2\n",
      "    Uninstalling async-timeout-4.0.2:\n",
      "      Successfully uninstalled async-timeout-4.0.2\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 3.6.2\n",
      "    Uninstalling anyio-3.6.2:\n",
      "      Successfully uninstalled anyio-3.6.2\n",
      "  Attempting uninstall: typing-inspect\n",
      "    Found existing installation: typing-inspect 0.8.0\n",
      "    Uninstalling typing-inspect-0.8.0:\n",
      "      Successfully uninstalled typing-inspect-0.8.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.0\n",
      "    Uninstalling torch-1.12.0:\n",
      "      Successfully uninstalled torch-1.12.0\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.2\n",
      "    Uninstalling tokenizers-0.13.2:\n",
      "      Successfully uninstalled tokenizers-0.13.2\n",
      "  Attempting uninstall: threadpoolctl\n",
      "    Found existing installation: threadpoolctl 3.1.0\n",
      "    Uninstalling threadpoolctl-3.1.0:\n",
      "      Successfully uninstalled threadpoolctl-3.1.0\n",
      "  Attempting uninstall: soupsieve\n",
      "    Found existing installation: soupsieve 2.3.1\n",
      "    Uninstalling soupsieve-2.3.1:\n",
      "      Successfully uninstalled soupsieve-2.3.1\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 61.2.0\n",
      "    Uninstalling setuptools-61.2.0:\n",
      "      Successfully uninstalled setuptools-61.2.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.10.0\n",
      "    Uninstalling scipy-1.10.0:\n",
      "      Successfully uninstalled scipy-1.10.0\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2022.3.15\n",
      "    Uninstalling regex-2022.3.15:\n",
      "      Successfully uninstalled regex-2022.3.15\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2022.7.1\n",
      "    Uninstalling pytz-2022.7.1:\n",
      "      Successfully uninstalled pytz-2022.7.1\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 9.5.0\n",
      "    Uninstalling Pillow-9.5.0:\n",
      "      Successfully uninstalled Pillow-9.5.0\n",
      "  Attempting uninstall: marshmallow\n",
      "    Found existing installation: marshmallow 3.19.0\n",
      "    Uninstalling marshmallow-3.19.0:\n",
      "      Successfully uninstalled marshmallow-3.19.0\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 0.17.2\n",
      "    Uninstalling httpcore-0.17.2:\n",
      "      Successfully uninstalled httpcore-0.17.2\n",
      "  Attempting uninstall: greenlet\n",
      "    Found existing installation: greenlet 1.1.2\n",
      "    Uninstalling greenlet-1.1.2:\n",
      "      Successfully uninstalled greenlet-1.1.2\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.9.0\n",
      "    Uninstalling google-auth-2.9.0:\n",
      "      Successfully uninstalled google-auth-2.9.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.4\n",
      "    Uninstalling dill-0.3.4:\n",
      "      Successfully uninstalled dill-0.3.4\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.0.4\n",
      "    Uninstalling click-8.0.4:\n",
      "      Successfully uninstalled click-8.0.4\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.8.3\n",
      "    Uninstalling aiohttp-3.8.3:\n",
      "      Successfully uninstalled aiohttp-3.8.3\n",
      "  Attempting uninstall: xxhash\n",
      "    Found existing installation: xxhash 0.0.0\n",
      "    Uninstalling xxhash-0.0.0:\n",
      "      Successfully uninstalled xxhash-0.0.0\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.14.1\n",
      "    Uninstalling wrapt-1.14.1:\n",
      "      Successfully uninstalled wrapt-1.14.1\n",
      "  Attempting uninstall: Werkzeug\n",
      "    Found existing installation: Werkzeug 2.1.2\n",
      "    Uninstalling Werkzeug-2.1.2:\n",
      "      Successfully uninstalled Werkzeug-2.1.2\n",
      "  Attempting uninstall: websocket-client\n",
      "    Found existing installation: websocket-client 1.5.1\n",
      "    Uninstalling websocket-client-1.5.1:\n",
      "      Successfully uninstalled websocket-client-1.5.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.26.0\n",
      "    Uninstalling transformers-4.26.0:\n",
      "      Successfully uninstalled transformers-4.26.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.13.0\n",
      "    Uninstalling torchvision-0.13.0:\n",
      "      Successfully uninstalled torchvision-0.13.0\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.6.1\n",
      "    Uninstalling tensorboard-data-server-0.6.1:\n",
      "      Successfully uninstalled tensorboard-data-server-0.6.1\n",
      "  Attempting uninstall: SQLAlchemy\n",
      "    Found existing installation: SQLAlchemy 1.4.40\n",
      "    Uninstalling SQLAlchemy-1.4.40:\n",
      "      Successfully uninstalled SQLAlchemy-1.4.40\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.2\n",
      "    Uninstalling scikit-learn-0.24.2:\n",
      "      Successfully uninstalled scikit-learn-0.24.2\n",
      "  Attempting uninstall: ruamel.yaml.clib\n",
      "    Found existing installation: ruamel.yaml.clib 0.2.7\n",
      "    Uninstalling ruamel.yaml.clib-0.2.7:\n",
      "      Successfully uninstalled ruamel.yaml.clib-0.2.7\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.0.4\n",
      "    Uninstalling pyparsing-3.0.4:\n",
      "      Successfully uninstalled pyparsing-3.0.4\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 11.0.0\n",
      "    Uninstalling pyarrow-11.0.0:\n",
      "      Successfully uninstalled pyarrow-11.0.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.4\n",
      "    Uninstalling protobuf-3.19.4:\n",
      "      Successfully uninstalled protobuf-3.19.4\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.4.3\n",
      "    Uninstalling pandas-1.4.3:\n",
      "      Successfully uninstalled pandas-1.4.3\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.14\n",
      "    Uninstalling multiprocess-0.70.14:\n",
      "      Successfully uninstalled multiprocess-0.70.14\n",
      "  Attempting uninstall: Markdown\n",
      "    Found existing installation: Markdown 3.3.7\n",
      "    Uninstalling Markdown-3.3.7:\n",
      "      Successfully uninstalled Markdown-3.3.7\n",
      "  Attempting uninstall: llvmlite\n",
      "    Found existing installation: llvmlite 0.39.1\n",
      "    Uninstalling llvmlite-0.39.1:\n",
      "      Successfully uninstalled llvmlite-0.39.1\n",
      "  Attempting uninstall: kiwisolver\n",
      "    Found existing installation: kiwisolver 1.4.4\n",
      "    Uninstalling kiwisolver-1.4.4:\n",
      "      Successfully uninstalled kiwisolver-1.4.4\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.24.1\n",
      "    Uninstalling httpx-0.24.1:\n",
      "      Successfully uninstalled httpx-0.24.1\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.47.0\n",
      "    Uninstalling grpcio-1.47.0:\n",
      "      Successfully uninstalled grpcio-1.47.0\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 0.4.6\n",
      "    Uninstalling google-auth-oauthlib-0.4.6:\n",
      "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
      "  Attempting uninstall: fonttools\n",
      "    Found existing installation: fonttools 4.35.0\n",
      "    Uninstalling fonttools-4.35.0:\n",
      "      Successfully uninstalled fonttools-4.35.0\n",
      "  Attempting uninstall: dataclasses-json\n",
      "    Found existing installation: dataclasses-json 0.5.7\n",
      "    Uninstalling dataclasses-json-0.5.7:\n",
      "      Successfully uninstalled dataclasses-json-0.5.7\n",
      "  Attempting uninstall: cycler\n",
      "    Found existing installation: cycler 0.11.0\n",
      "    Uninstalling cycler-0.11.0:\n",
      "      Successfully uninstalled cycler-0.11.0\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.11.1\n",
      "    Uninstalling beautifulsoup4-4.11.1:\n",
      "      Successfully uninstalled beautifulsoup4-4.11.1\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 1.1.0\n",
      "    Uninstalling absl-py-1.1.0:\n",
      "      Successfully uninstalled absl-py-1.1.0\n",
      "  Attempting uninstall: uvicorn\n",
      "    Found existing installation: uvicorn 0.18.2\n",
      "    Uninstalling uvicorn-0.18.2:\n",
      "      Successfully uninstalled uvicorn-0.18.2\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.9.1\n",
      "    Uninstalling tensorboard-2.9.1:\n",
      "      Successfully uninstalled tensorboard-2.9.1\n",
      "  Attempting uninstall: ruamel.yaml\n",
      "    Found existing installation: ruamel.yaml 0.17.21\n",
      "    Uninstalling ruamel.yaml-0.17.21:\n",
      "      Successfully uninstalled ruamel.yaml-0.17.21\n",
      "  Attempting uninstall: numba\n",
      "    Found existing installation: numba 0.56.4\n",
      "    Uninstalling numba-0.56.4:\n",
      "      Successfully uninstalled numba-0.56.4\n",
      "  Attempting uninstall: nest-asyncio\n",
      "    Found existing installation: nest-asyncio 1.5.5\n",
      "    Uninstalling nest-asyncio-1.5.5:\n",
      "      Successfully uninstalled nest-asyncio-1.5.5\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.5.3\n",
      "    Uninstalling matplotlib-3.5.3:\n",
      "      Successfully uninstalled matplotlib-3.5.3\n",
      "  Attempting uninstall: lxml\n",
      "    Found existing installation: lxml 4.9.1\n",
      "    Uninstalling lxml-4.9.1:\n",
      "      Successfully uninstalled lxml-4.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eae35b",
   "metadata": {
    "papermill": {
     "duration": 0.008621,
     "end_time": "2023-11-10T03:16:43.912458",
     "exception": false,
     "start_time": "2023-11-10T03:16:43.903837",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Required Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d693d8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T03:16:43.931459Z",
     "iopub.status.busy": "2023-11-10T03:16:43.931185Z",
     "iopub.status.idle": "2023-11-10T03:16:56.788686Z",
     "shell.execute_reply": "2023-11-10T03:16:56.787466Z"
    },
    "papermill": {
     "duration": 12.868505,
     "end_time": "2023-11-10T03:16:56.789952",
     "exception": true,
     "start_time": "2023-11-10T03:16:43.921447",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'joblib' from 'sklearn.externals' (/opt/conda/lib/python3.10/site-packages/sklearn/externals/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprofanity_check\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m predict_prob\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n",
      "File \u001b[0;32m/kaggle/working/workshop/profanity_check/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprofanity_check\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m predict, predict_prob\n\u001b[1;32m      2\u001b[0m __version__\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/kaggle/working/workshop/profanity_check/profanity_check.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpkg_resources\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m joblib\n\u001b[1;32m      5\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(pkg_resources\u001b[38;5;241m.\u001b[39mresource_filename(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprofanity_check\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/vectorizer.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(pkg_resources\u001b[38;5;241m.\u001b[39mresource_filename(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprofanity_check\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/model.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'joblib' from 'sklearn.externals' (/opt/conda/lib/python3.10/site-packages/sklearn/externals/__init__.py)"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set flag for training environment\n",
    "TRAINING = True\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from profanity_check import predict_prob\n",
    "\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10002bc4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Dataset\n",
    "\n",
    "Training dataset: Text content and metadata for LinkedIn posts during 2021. These posts were collected from the\n",
    "internet and correspond to a wide array of identified \"influencers\".\n",
    "\n",
    "Source: https://www.kaggle.com/datasets/shreyasajal/linkedin-influencers-data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddce8f2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Import Dataset\n",
    "\n",
    "To the right of this page, go to \"Add Data\" and search for \"LinkedIn Influencers' Data\". Hit the \"+\" icon to add the data to your `/kaggle/input` data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda96b92-1e29-433a-8d76-4db8083d201c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wget https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/data/influencers_data.csv -o influencers_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadbc183",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:00.121461Z",
     "iopub.status.busy": "2023-11-10T01:50:00.120562Z",
     "iopub.status.idle": "2023-11-10T01:50:03.834591Z",
     "shell.execute_reply": "2023-11-10T01:50:03.831388Z",
     "shell.execute_reply.started": "2023-11-10T01:50:00.121405Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in from public S3 Location\n",
    "df = pd.read_csv(\"influencers_data.csv\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc32b911",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:04.892358Z",
     "iopub.status.busy": "2023-11-10T01:50:04.891751Z",
     "iopub.status.idle": "2023-11-10T01:50:04.997907Z",
     "shell.execute_reply": "2023-11-10T01:50:04.995978Z",
     "shell.execute_reply.started": "2023-11-10T01:50:04.892318Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26776941",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:05.830658Z",
     "iopub.status.busy": "2023-11-10T01:50:05.828425Z",
     "iopub.status.idle": "2023-11-10T01:50:05.843490Z",
     "shell.execute_reply": "2023-11-10T01:50:05.841077Z",
     "shell.execute_reply.started": "2023-11-10T01:50:05.830586Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbda2b6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "There are approximately ~70 LinkedIn influencers in this dataset. Let's take a look at\n",
    "how many posts we have data on for each influencer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa9da0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:06.952167Z",
     "iopub.status.busy": "2023-11-10T01:50:06.951656Z",
     "iopub.status.idle": "2023-11-10T01:50:08.203141Z",
     "shell.execute_reply": "2023-11-10T01:50:08.201212Z",
     "shell.execute_reply.started": "2023-11-10T01:50:06.952131Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# count how many records we have for influencers\n",
    "df.name.value_counts().sort_values().plot(kind='barh', figsize=(8,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8051f3f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Create Small Sample DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34014e18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:09.248160Z",
     "iopub.status.busy": "2023-11-10T01:50:09.247639Z",
     "iopub.status.idle": "2023-11-10T01:50:09.275727Z",
     "shell.execute_reply": "2023-11-10T01:50:09.274577Z",
     "shell.execute_reply.started": "2023-11-10T01:50:09.248122Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df = df.sample(100)[['name', 'headline', 'content']]\n",
    "\n",
    "# drop records with missing content\n",
    "sample_df = sample_df.dropna()\n",
    "\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c476dbbd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Custom Functions\n",
    "\n",
    "Set up a custom function to help with the data transformation. \n",
    "\n",
    "```\n",
    "def custom_function(df, text_columns):\n",
    "    \n",
    "    # For text cleaning based functions use: \n",
    "    for column in text_columns: \n",
    "        df[column] = # add change code here \n",
    "    \n",
    "    #For an example of how to filter the df see number_proportion_filter.py or numeric_filer.py \n",
    "    \n",
    "    return df\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897a82e5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 🙈 Profanity Check\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f52648",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:11.258071Z",
     "iopub.status.busy": "2023-11-10T01:50:11.257532Z",
     "iopub.status.idle": "2023-11-10T01:50:11.307660Z",
     "shell.execute_reply": "2023-11-10T01:50:11.306721Z",
     "shell.execute_reply.started": "2023-11-10T01:50:11.258025Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from profanity_check import predict_prob\n",
    "\n",
    "threshold = 0.9\n",
    "\n",
    "sample_df[\"profanity\"] = predict_prob(sample_df[\"content\"])\n",
    "\n",
    "sample_df = sample_df[sample_df[\"profanity\"] < threshold]\n",
    "sample_df = sample_df.reset_index(drop=True)\n",
    "sample_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653368f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:11.945721Z",
     "iopub.status.busy": "2023-11-10T01:50:11.944630Z",
     "iopub.status.idle": "2023-11-10T01:50:12.298906Z",
     "shell.execute_reply": "2023-11-10T01:50:12.297561Z",
     "shell.execute_reply.started": "2023-11-10T01:50:11.945672Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df['profanity'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e7e842",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 🧐 Text Quality Check\n",
    "\n",
    "## Flesch Grade reading level\n",
    "\n",
    "Depending on the task at hand, sometimes it's advantages to evaluate text and\n",
    "language based on it's estimated reading level. One popular way to do this \n",
    "is with the Flesch-Kincaid grade level.  This is a grade formula in that a score\n",
    "of 9.3 means that a ninth grader would be able to read the document.\n",
    "\n",
    "The wikipedia article on this technique does a fair job explaining it:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b1374f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Using the `textstat` python linvrary, we can approximate the reading level and\n",
    "set our thresholds for what level we find ideal for our downstream NLP task.\n",
    "\n",
    "In this case, we will limit the reading level between `2` and `10`, but many\n",
    "other ranges are certainly possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2587d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:14.284426Z",
     "iopub.status.busy": "2023-11-10T01:50:14.283577Z",
     "iopub.status.idle": "2023-11-10T01:50:14.562179Z",
     "shell.execute_reply": "2023-11-10T01:50:14.560781Z",
     "shell.execute_reply.started": "2023-11-10T01:50:14.284382Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import textstat \n",
    "\n",
    "min_grade_level = 2\n",
    "max_grade_level = 10\n",
    "\n",
    "def check_quality(df, text_column):\n",
    "    df[\"flesch_grade\"] = df[text_column].apply(textstat.flesch_kincaid_grade)\n",
    "    return df\n",
    "\n",
    "cleaned = check_quality(sample_df, 'content')\n",
    "\n",
    "# Filter to only those records with content between 2 and 10 grade level\n",
    "cleaned = cleaned[(cleaned[\"flesch_grade\"] >= min_grade_level) & (cleaned[\"flesch_grade\"] <= max_grade_level)]\n",
    "cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279d4b96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:15.487127Z",
     "iopub.status.busy": "2023-11-10T01:50:15.486591Z",
     "iopub.status.idle": "2023-11-10T01:50:15.816502Z",
     "shell.execute_reply": "2023-11-10T01:50:15.815114Z",
     "shell.execute_reply.started": "2023-11-10T01:50:15.487088Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df['flesch_grade'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dc3a21",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Run Preparation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebec03f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:17.353282Z",
     "iopub.status.busy": "2023-11-10T01:50:17.352386Z",
     "iopub.status.idle": "2023-11-10T01:50:32.076379Z",
     "shell.execute_reply": "2023-11-10T01:50:32.075075Z",
     "shell.execute_reply.started": "2023-11-10T01:50:17.353237Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic ETL\n",
    "\n",
    "# Read in some additional utility functions\n",
    "\n",
    "def remove_trailing_ws(df, text_columns):\n",
    "    # For text cleaning based functions use: \n",
    "    for column in text_columns: \n",
    "        df[column] = df[column].str.strip()\n",
    "    return df\n",
    "\n",
    "def length_check_func(df,text_column, minLength,maxLength):\n",
    "    df = df[(df[text_column].str.len() > int(minLength))]\n",
    "    df = df[(df[text_column].str.len() < int(maxLength))]\n",
    "    return df\n",
    "\n",
    "def string_replace(df, text_columns, input, output):\n",
    "    # For text cleaning based functions use: \n",
    "    for column in text_columns: \n",
    "        df[column] = df[column].str.replace(input, output)\n",
    "    return df\n",
    "\n",
    "# Keep Relevant Columns\n",
    "cleaned = df[['name', 'headline', 'about', 'content', 'reactions']]\n",
    "\n",
    "# Dropping missing\n",
    "cleaned = cleaned.dropna()\n",
    "\n",
    "\n",
    "# Text Cleaning (Minimal)\n",
    "cleaned = string_replace(cleaned, ['content'], '…see more', '')\n",
    "cleaned = remove_trailing_ws(cleaned, ['content'])\n",
    "\n",
    "max_len = 10000\n",
    "min_len = 1\n",
    "\n",
    "cleaned = length_check_func(cleaned, 'content', min_len, max_len)\n",
    "\n",
    "\n",
    "# Set thresholds\n",
    "\n",
    "# Profanity Threshold\n",
    "profanity_threshold = 0.9\n",
    "\n",
    "# Target Reading Level\n",
    "min_grade_level = 2\n",
    "max_grade_level = 10\n",
    "\n",
    "# Predict Probability of Profane Language\n",
    "cleaned[\"profanity\"] = predict_prob(cleaned[\"content\"])\n",
    "\n",
    "# Filter out Profane Language\n",
    "cleaned = cleaned[cleaned[\"profanity\"] < profanity_threshold]\n",
    "cleaned = cleaned.reset_index(drop=True)\n",
    "\n",
    "# Determine Reading Level\n",
    "cleaned = check_quality(cleaned, 'content')\n",
    "\n",
    "# Filter to only those records with content between a 2nd and 10th grader\n",
    "cleaned = cleaned[(cleaned[\"flesch_grade\"] >= min_grade_level) & (cleaned[\"flesch_grade\"] <= max_grade_level)]\n",
    "cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dedfa86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:32.079081Z",
     "iopub.status.busy": "2023-11-10T01:50:32.078554Z",
     "iopub.status.idle": "2023-11-10T01:50:32.085937Z",
     "shell.execute_reply": "2023-11-10T01:50:32.084738Z",
     "shell.execute_reply.started": "2023-11-10T01:50:32.079048Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_dropped = df.shape[0] - cleaned.shape[0]\n",
    "\n",
    "print(f\"Lost a total of {num_dropped} records, or about {round(num_dropped/df.shape[0], 2) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82311a97",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# One Step Further\n",
    "\n",
    "If we really want to talk like an influencer, perhaps we should also additionally limit the number of records\n",
    "by the number of reactions the posts got. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb86e51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:34.300043Z",
     "iopub.status.busy": "2023-11-10T01:50:34.299545Z",
     "iopub.status.idle": "2023-11-10T01:50:34.326470Z",
     "shell.execute_reply": "2023-11-10T01:50:34.325042Z",
     "shell.execute_reply.started": "2023-11-10T01:50:34.300006Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned.sort_values('reactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205e140d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We can see that the number of reactions these posts from 'influencers' received appear to go \n",
    "from zero (yikes!) all the way to over 330k. Noticeably, that single post with ~330k reactions\n",
    "is simply `Helen is my kinda lady`.\n",
    "\n",
    "The fact this post received so much attention given it's lack of context is also \n",
    "a warning that there could be exogenous latent variables, such as current events, pop culture, etc, that could be driving \n",
    "the number of reactions, not necessarily the content itself. \n",
    "\n",
    "Let's be just a little more analytical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd069513",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:39.650715Z",
     "iopub.status.busy": "2023-11-10T01:50:39.649090Z",
     "iopub.status.idle": "2023-11-10T01:50:39.661174Z",
     "shell.execute_reply": "2023-11-10T01:50:39.659715Z",
     "shell.execute_reply.started": "2023-11-10T01:50:39.650652Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify the 90th percentile of reactions over all posts\n",
    "p90 = np.quantile(cleaned.reactions, 0.9)\n",
    "\n",
    "print(f'The 90th percentile is {p90} reactions.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcfdc74",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "For our purposes, let's focus on the top \"performing\" content to fine tune our model on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11752a02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:41.078148Z",
     "iopub.status.busy": "2023-11-10T01:50:41.077488Z",
     "iopub.status.idle": "2023-11-10T01:50:41.090022Z",
     "shell.execute_reply": "2023-11-10T01:50:41.088789Z",
     "shell.execute_reply.started": "2023-11-10T01:50:41.078098Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned = cleaned[cleaned.reactions > p90]\n",
    "cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f711c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We're now left with 2,127 high-quality data points to experiment with \n",
    "fine tuning on. \n",
    "\n",
    "Finally, let's ask h2oGPT to provide a title for our LinkedIn Influencer content.\n",
    "This process is called `zero-shot text generation` (more on this later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b8475",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:51:12.369670Z",
     "iopub.status.busy": "2023-11-10T01:51:12.369182Z",
     "iopub.status.idle": "2023-11-10T01:51:21.610939Z",
     "shell.execute_reply": "2023-11-10T01:51:21.609347Z",
     "shell.execute_reply.started": "2023-11-10T01:51:12.369634Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df = cleaned.sample(5)\n",
    "\n",
    "from gradio_client import Client\n",
    "import ast\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "HOST_URL = \"https://gpt-genai.h2o.ai/\"\n",
    "GPT_KEY = \"f74f043e-45fc-4dfe-9c33-55a4720427f6\"\n",
    "    \n",
    "client = Client(HOST_URL)\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def generate_title(content):\n",
    "    \n",
    "    #try:\n",
    "    summarize_prompt = 'You are a helpful, respectful and honest assistant the specializes in generating accurate titles of LinkedIn posts. Provide a title for the following post. The title should be a single sentence, not using bullet points. Only include the title in the response. The LinkedIn post is: ' + content\n",
    "    kwargs = dict(\n",
    "        instruction_nochat=summarize_prompt, \n",
    "        h2ogpt_key = GPT_KEY)\n",
    "    \n",
    "    response = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')\n",
    "    reply = ast.literal_eval(response)['response']\n",
    "    #except:\n",
    "        #reply = 'NA'\n",
    "    return reply\n",
    "        \n",
    "sample_df['title'] = sample_df.progress_apply(lambda row :   generate_title(row['content']), axis=1)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8040bb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:51:22.779132Z",
     "iopub.status.busy": "2023-11-10T01:51:22.778601Z",
     "iopub.status.idle": "2023-11-10T01:51:22.788148Z",
     "shell.execute_reply": "2023-11-10T01:51:22.786648Z",
     "shell.execute_reply.started": "2023-11-10T01:51:22.779093Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pprint(f'CONTENT:')\n",
    "sample_df['content'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001acb4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:51:23.762650Z",
     "iopub.status.busy": "2023-11-10T01:51:23.761810Z",
     "iopub.status.idle": "2023-11-10T01:51:23.770735Z",
     "shell.execute_reply": "2023-11-10T01:51:23.769707Z",
     "shell.execute_reply.started": "2023-11-10T01:51:23.762609Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'TITLE:')\n",
    "sample_df['title'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf5c7ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:51:24.548964Z",
     "iopub.status.busy": "2023-11-10T01:51:24.547836Z",
     "iopub.status.idle": "2023-11-10T01:51:24.574766Z",
     "shell.execute_reply": "2023-11-10T01:51:24.573266Z",
     "shell.execute_reply.started": "2023-11-10T01:51:24.548917Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional alternative for fine tuning - create an instruction.\n",
    "cleaned['instruction'] = ('Write a LinkedIn post in the style of an influencer whom has the title of '\n",
    "  + cleaned['headline'] + ' and can be described by the following: ' \n",
    "  + cleaned['about'])\n",
    "\n",
    "cleaned.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae17bc7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Output Dataset\n",
    "\n",
    "Now we're ready to store our data set out and experiment with fine-tuning\n",
    "in Lab # 2 with H2O LLM Studio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b25ad",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-11-10T01:52:01.078715Z",
     "iopub.status.busy": "2023-11-10T01:52:01.078185Z",
     "iopub.status.idle": "2023-11-10T03:01:13.098104Z",
     "shell.execute_reply": "2023-11-10T03:01:13.096087Z",
     "shell.execute_reply.started": "2023-11-10T01:52:01.078668Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# WARNING! This could take a very long time using the public facing h2oGPT endpoint. ##\n",
    "#######################################################################################\n",
    "\n",
    "# Apply to full data set\n",
    "cleaned['title'] = cleaned.progress_apply(lambda row :   generate_title(row['content']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c630f9f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T03:12:13.080002Z",
     "iopub.status.busy": "2023-11-10T03:12:13.079498Z",
     "iopub.status.idle": "2023-11-10T03:12:13.513635Z",
     "shell.execute_reply": "2023-11-10T03:12:13.512360Z",
     "shell.execute_reply.started": "2023-11-10T03:12:13.079963Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Output locally\n",
    "cleaned.to_csv('influencers_data_prepared.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc2bdd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 38.069262,
   "end_time": "2023-11-10T03:16:58.522503",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-10T03:16:20.453241",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
