{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33ce6b46",
   "metadata": {
    "papermill": {
     "duration": 0.010207,
     "end_time": "2023-11-10T03:16:23.408677",
     "exception": false,
     "start_time": "2023-11-10T03:16:23.398470",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **LAB 1: DATA PREPARATION**\n",
    "\n",
    "In this notebook, we will explore how do prepare data for downstream NLP tasks.  \n",
    "\n",
    "A non-exhaustive list tasks related to NLP and Generative AI that can benefit from this data\n",
    "preparation is:\n",
    "\n",
    "- Question & Answer\n",
    "- Text Summarization\n",
    "- Instruct Tuning\n",
    "- Human-Bot Conversations\n",
    "- Continued PreTraining\n",
    "\n",
    "\n",
    "For each of these tasks, there are subtle differences in how one might want to prepare the data.\n",
    "Some of the key steps one might use here for data prep are also applicable to \n",
    "applying guardrails to LLMs (e.g., `Profanity Check` and `Toxicity Detection`)\n",
    "\n",
    "More on guardrails in Lab #5, but for now let's import our required python \n",
    "libraries and get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f976843",
   "metadata": {
    "papermill": {
     "duration": 0.00715,
     "end_time": "2023-11-10T03:16:23.423640",
     "exception": false,
     "start_time": "2023-11-10T03:16:23.416490",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Step 1: Persistent Python environment\n",
    "\n",
    "[Implementation](https://www.kaggle.com/code/kononenko/pip-install-once)\n",
    "[pip-install-forever](https://www.kaggle.com/code/samsammurphy/pip-install-forever)\n",
    "\n",
    "- Adjust the session persistence settings. \n",
    "  - Go to Notebook options (right pane) and set PERSISTENCE to Files only. You can also set it to Variables and Files, if you need persistent variables for your own reasons.\n",
    "  - Install in target directory: `pip install -r requirements.txt --target=/kaggle/working/workshop`\n",
    "  - Next:\n",
    "    1. Import sys: `import sys`\n",
    "    2. Add path: `sys.path.append(\"/kaggle/working/workshop\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f8e8c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 1.562386,
     "end_time": "2023-11-10T03:16:24.993330",
     "exception": false,
     "start_time": "2023-11-10T03:16:23.430944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting absl-py==2.0.0\n",
      "  Using cached absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "Collecting accelerate==0.24.0\n",
      "  Using cached accelerate-0.24.0-py3-none-any.whl (260 kB)\n",
      "Collecting aiobotocore==2.7.0\n",
      "  Using cached aiobotocore-2.7.0-py3-none-any.whl (73 kB)\n",
      "Collecting aiohttp==3.8.5\n",
      "  Using cached aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "Collecting aioitertools==0.11.0\n",
      "  Using cached aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: aiosignal==1.3.1 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 6)) (1.3.1)\n",
      "Collecting alembic==1.12.1\n",
      "  Using cached alembic-1.12.1-py3-none-any.whl (226 kB)\n",
      "Collecting alt-profanity-check==1.3.1\n",
      "  Using cached alt-profanity-check-1.3.1.tar.gz (1.9 MB)\n",
      "Collecting annoy==1.17.3\n",
      "  Using cached annoy-1.17.3.tar.gz (647 kB)\n",
      "Collecting anyio==4.0.0\n",
      "  Using cached anyio-4.0.0-py3-none-any.whl (83 kB)\n",
      "Collecting async-timeout==4.0.3\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting attrs==23.1.0\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting beautifulsoup4==4.12.2\n",
      "  Using cached beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "Collecting blessed==1.20.0\n",
      "  Using cached blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
      "Collecting blinker==1.7.0\n",
      "  Using cached blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting botocore==1.31.64\n",
      "  Using cached botocore-1.31.64-py3-none-any.whl (11.3 MB)\n",
      "Collecting bs4==0.0.1\n",
      "  Using cached bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Collecting cachetools==5.3.2\n",
      "  Using cached cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Collecting certifi==2023.7.22\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "Collecting charset-normalizer==3.3.1\n",
      "  Using cached charset_normalizer-3.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
      "Collecting click==8.1.7\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Collecting cloudpickle==2.2.1\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting colorama==0.4.6\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting contourpy==1.1.1\n",
      "  Using cached contourpy-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "Collecting cycler==0.12.1\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting databricks-cli==0.18.0\n",
      "  Using cached databricks_cli-0.18.0-py2.py3-none-any.whl (150 kB)\n",
      "Collecting dataclasses-json==0.5.14\n",
      "  Using cached dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
      "Collecting datasets==2.14.6\n",
      "  Using cached datasets-2.14.6-py3-none-any.whl (493 kB)\n",
      "Collecting dill==0.3.7\n",
      "  Using cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Collecting docker==6.1.3\n",
      "  Using cached docker-6.1.3-py3-none-any.whl (148 kB)\n",
      "Requirement already satisfied: entrypoints==0.4 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 31)) (0.4)\n",
      "Collecting evaluate==0.4.1\n",
      "  Using cached evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "Collecting fastapi==0.96.0\n",
      "  Using cached fastapi-0.96.0-py3-none-any.whl (57 kB)\n",
      "Collecting filelock==3.12.4\n",
      "  Using cached filelock-3.12.4-py3-none-any.whl (11 kB)\n",
      "Collecting Flask==3.0.0\n",
      "  Using cached flask-3.0.0-py3-none-any.whl (99 kB)\n",
      "Collecting fonttools==4.43.1\n",
      "  Downloading fonttools-4.43.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 659 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting frozenlist==1.4.0\n",
      "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
      "\u001b[K     |████████████████████████████████| 225 kB 758 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec==2023.10.0\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[K     |████████████████████████████████| 166 kB 952 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gitdb==4.0.11\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 480 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting GitPython==3.1.40\n",
      "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
      "\u001b[K     |████████████████████████████████| 190 kB 930 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth==2.23.3\n",
      "  Downloading google_auth-2.23.3-py2.py3-none-any.whl (182 kB)\n",
      "\u001b[K     |████████████████████████████████| 182 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib==1.1.0\n",
      "  Downloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting gradio_client==0.3.0\n",
      "  Downloading gradio_client-0.3.0-py3-none-any.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 461 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting greenlet==3.0.1\n",
      "  Downloading greenlet-3.0.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (613 kB)\n",
      "\u001b[K     |████████████████████████████████| 613 kB 841 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio==1.59.0\n",
      "  Downloading grpcio-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.3 MB 431 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gunicorn==21.2.0\n",
      "  Downloading gunicorn-21.2.0-py3-none-any.whl (80 kB)\n",
      "\u001b[K     |████████████████████████████████| 80 kB 997 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h11==0.14.0 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 47)) (0.14.0)\n",
      "Collecting h2o-authn==1.1.0\n",
      "  Downloading h2o_authn-1.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting h2o-mlops==0.62.1a1\n",
      "  Downloading h2o_mlops-0.62.1a1-py3-none-any.whl (771 kB)\n",
      "\u001b[K     |████████████████████████████████| 771 kB 608 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h2o-wave==1.0.0\n",
      "  Downloading h2o_wave-1.0.0-py3-none-manylinux1_x86_64.whl (12.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.1 MB 712 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h2ogpte==0.9.2\n",
      "  Downloading h2ogpte-0.9.2-py3-none-any.whl (7.8 kB)\n",
      "Collecting httpcore==0.16.3\n",
      "  Using cached httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
      "Collecting httpx==0.23.3\n",
      "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
      "\u001b[K     |████████████████████████████████| 71 kB 574 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting huggingface-hub==0.17.3\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[K     |████████████████████████████████| 295 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna==3.4\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 77 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting inquirer==3.1.3\n",
      "  Using cached inquirer-3.1.3-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: itsdangerous==2.1.2 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 57)) (2.1.2)\n",
      "Collecting Jinja2==3.1.2\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath==1.0.1\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting joblib==1.3.2\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[K     |████████████████████████████████| 302 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver==1.4.5\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 756 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting langchain==0.0.251\n",
      "  Downloading langchain-0.0.251-py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 308 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting langsmith==0.0.53\n",
      "  Downloading langsmith-0.0.53-py3-none-any.whl (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 813 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting llvmlite==0.41.1\n",
      "  Downloading llvmlite-0.41.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 43.6 MB 24 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting lxml==4.9.3\n",
      "  Downloading lxml-4.9.3-cp310-cp310-manylinux_2_28_x86_64.whl (7.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.9 MB 220 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Mako==1.2.4 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 66)) (1.2.4)\n",
      "Collecting Markdown==3.5\n",
      "  Downloading Markdown-3.5-py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 1.4 MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting MarkupSafe==2.1.3\n",
      "  Downloading MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting marshmallow==3.20.1\n",
      "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 770 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting matplotlib==3.8.0\n",
      "  Downloading matplotlib-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.6 MB 79 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting mlflow==2.8.0\n",
      "  Downloading mlflow-2.8.0-py3-none-any.whl (19.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.0 MB 35 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting mpmath==1.3.0\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Requirement already satisfied: multidict==6.0.4 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 73)) (6.0.4)\n",
      "Collecting multiprocess==0.70.15\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[K     |████████████████████████████████| 134 kB 480 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: mypy-extensions==1.0.0 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 75)) (1.0.0)\n",
      "Collecting nemo-toolkit==1.21.0\n",
      "  Downloading nemo_toolkit-1.21.0-py3-none-any.whl (2.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5 MB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nemoguardrails==0.5.0\n",
      "  Downloading nemoguardrails-0.5.0-py3-none-any.whl (13.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.9 MB 142 kB/s eta 0:00:01    |██▌                             | 1.1 MB 2.3 MB/s eta 0:00:06\n",
      "\u001b[?25hCollecting nest-asyncio==1.5.6\n",
      "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
      "Collecting networkx==3.2\n",
      "  Downloading networkx-3.2-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 981 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nltk==3.8.1 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 80)) (3.8.1)\n",
      "Collecting numba==0.58.1\n",
      "  Downloading numba-0.58.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numexpr==2.8.7\n",
      "  Downloading numexpr-2.8.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384 kB)\n",
      "\u001b[K     |████████████████████████████████| 384 kB 371 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy==1.23.5\n",
      "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.1 MB 77 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting oauthlib==3.2.2\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 163 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting onnx==1.15.0\n",
      "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7 MB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting openai==0.28.1\n",
      "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 829 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting openapi-schema-pydantic==1.2.4\n",
      "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 2.4 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pandas==2.1.1\n",
      "  Downloading pandas-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.3 MB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Pillow==10.1.0\n",
      "  Downloading Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 356 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting portalocker==2.8.2\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Collecting protobuf==4.23.4\n",
      "  Using cached protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "Collecting pyarrow==13.0.0\n",
      "  Downloading pyarrow-13.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 40.0 MB 3.9 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1==0.5.0\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 335 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules==0.3.0\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic==1.10.13\n",
      "  Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 175 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyJWT==2.8.0\n",
      "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Collecting pyparsing==3.1.1\n",
      "  Downloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyphen==0.14.0\n",
      "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-dotenv==1.0.0\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting python-editor==1.0.4\n",
      "  Using cached python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
      "Collecting pytz==2023.3.post1\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[K     |████████████████████████████████| 502 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting PyYAML==6.0.1\n",
      "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "\u001b[K     |████████████████████████████████| 705 kB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting querystring-parser==1.2.4\n",
      "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting readchar==4.0.5\n",
      "  Using cached readchar-4.0.5-py3-none-any.whl (8.5 kB)\n",
      "Collecting regex==2023.10.3\n",
      "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[K     |████████████████████████████████| 773 kB 995 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests==2.31.0\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 636 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: requests-oauthlib==1.3.1 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 107)) (1.3.1)\n",
      "Requirement already satisfied: responses==0.18.0 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 108)) (0.18.0)\n",
      "Requirement already satisfied: rfc3986==1.5.0 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 109)) (1.5.0)\n",
      "Collecting rouge-score==0.1.2\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "Collecting rsa==4.9\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting ruamel.yaml==0.18.3\n",
      "  Downloading ruamel.yaml-0.18.3-py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 475 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ruamel.yaml.clib==0.2.8\n",
      "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
      "\u001b[K     |████████████████████████████████| 526 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3fs==2023.10.0\n",
      "  Downloading s3fs-2023.10.0-py3-none-any.whl (28 kB)\n",
      "Collecting sacrebleu==2.3.1\n",
      "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
      "\u001b[K     |████████████████████████████████| 118 kB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting safetensors==0.4.0\n",
      "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 520 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn==1.3.1\n",
      "  Downloading scikit_learn-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.8 MB 461 kB/s eta 0:00:0179 kB/s eta 0:00:120:11\n",
      "\u001b[?25hCollecting scipy==1.11.3\n",
      "  Downloading scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 36.4 MB 39 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting sentence-transformers==2.2.2\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 489 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece==0.1.99\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 645 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting simpleeval==0.9.13\n",
      "  Downloading simpleeval-0.9.13-py2.py3-none-any.whl (15 kB)\n",
      "Collecting smmap==5.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: sniffio==1.3.0 in /home/yashroff/miniconda3/envs/ame/lib/python3.10/site-packages (from -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt (line 123)) (1.3.0)\n",
      "Collecting soupsieve==2.5\n",
      "  Using cached soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Collecting SQLAlchemy==2.0.22\n",
      "  Downloading SQLAlchemy-2.0.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sqlparse==0.4.4\n",
      "  Downloading sqlparse-0.4.4-py3-none-any.whl (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 113 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting starlette==0.27.0\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "\u001b[K     |████████████████████████████████| 66 kB 3.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting sympy==1.12\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting tabulate==0.9.0\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting tenacity==8.2.3\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Collecting tensorboard==2.15.0\n",
      "  Downloading tensorboard-2.15.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server==0.7.2\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting text-unidecode==1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting textstat==0.7.3\n",
      "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl==3.2.0\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Collecting tokenizers==0.14.1\n",
      "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 790 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch==2.1.0\n",
      "  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[K     |████████████████████▌           | 429.7 MB 371 kB/s eta 0:10:472   |█                               | 21.1 MB 1.0 MB/s eta 0:10:36     |███▌                            | 73.9 MB 1.4 MB/s eta 0:07:03     |█████████████████▎              | 362.2 MB 1.2 MB/s eta 0:04:19[K     |██████████████████▊             | 392.4 MB 732 kB/s eta 0:06:20"
     ]
    }
   ],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/requirements_lab1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eae35b",
   "metadata": {
    "papermill": {
     "duration": 0.008621,
     "end_time": "2023-11-10T03:16:43.912458",
     "exception": false,
     "start_time": "2023-11-10T03:16:43.903837",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Required Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d693d8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T03:16:43.931459Z",
     "iopub.status.busy": "2023-11-10T03:16:43.931185Z",
     "iopub.status.idle": "2023-11-10T03:16:56.788686Z",
     "shell.execute_reply": "2023-11-10T03:16:56.787466Z"
    },
    "papermill": {
     "duration": 12.868505,
     "end_time": "2023-11-10T03:16:56.789952",
     "exception": true,
     "start_time": "2023-11-10T03:16:43.921447",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'joblib' from 'sklearn.externals' (/opt/conda/lib/python3.10/site-packages/sklearn/externals/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprofanity_check\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m predict_prob\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n",
      "File \u001b[0;32m/kaggle/working/workshop/profanity_check/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprofanity_check\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m predict, predict_prob\n\u001b[1;32m      2\u001b[0m __version__\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/kaggle/working/workshop/profanity_check/profanity_check.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpkg_resources\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m joblib\n\u001b[1;32m      5\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(pkg_resources\u001b[38;5;241m.\u001b[39mresource_filename(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprofanity_check\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/vectorizer.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(pkg_resources\u001b[38;5;241m.\u001b[39mresource_filename(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprofanity_check\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/model.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'joblib' from 'sklearn.externals' (/opt/conda/lib/python3.10/site-packages/sklearn/externals/__init__.py)"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set flag for training environment\n",
    "TRAINING = True\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from profanity_check import predict_prob\n",
    "\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10002bc4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Dataset\n",
    "\n",
    "Training dataset: Text content and metadata for LinkedIn posts during 2021. These posts were collected from the\n",
    "internet and correspond to a wide array of identified \"influencers\".\n",
    "\n",
    "Source: https://www.kaggle.com/datasets/shreyasajal/linkedin-influencers-data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddce8f2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Import Dataset\n",
    "\n",
    "To the right of this page, go to \"Add Data\" and search for \"LinkedIn Influencers' Data\". Hit the \"+\" icon to add the data to your `/kaggle/input` data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda96b92-1e29-433a-8d76-4db8083d201c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wget https://raw.githubusercontent.com/yasheshshroff/LLMworkshop/main/labs/data/influencers_data.csv -o influencers_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadbc183",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:00.121461Z",
     "iopub.status.busy": "2023-11-10T01:50:00.120562Z",
     "iopub.status.idle": "2023-11-10T01:50:03.834591Z",
     "shell.execute_reply": "2023-11-10T01:50:03.831388Z",
     "shell.execute_reply.started": "2023-11-10T01:50:00.121405Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in from public S3 Location\n",
    "df = pd.read_csv(\"influencers_data.csv\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc32b911",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:04.892358Z",
     "iopub.status.busy": "2023-11-10T01:50:04.891751Z",
     "iopub.status.idle": "2023-11-10T01:50:04.997907Z",
     "shell.execute_reply": "2023-11-10T01:50:04.995978Z",
     "shell.execute_reply.started": "2023-11-10T01:50:04.892318Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26776941",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:05.830658Z",
     "iopub.status.busy": "2023-11-10T01:50:05.828425Z",
     "iopub.status.idle": "2023-11-10T01:50:05.843490Z",
     "shell.execute_reply": "2023-11-10T01:50:05.841077Z",
     "shell.execute_reply.started": "2023-11-10T01:50:05.830586Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbda2b6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "There are approximately ~70 LinkedIn influencers in this dataset. Let's take a look at\n",
    "how many posts we have data on for each influencer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa9da0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:06.952167Z",
     "iopub.status.busy": "2023-11-10T01:50:06.951656Z",
     "iopub.status.idle": "2023-11-10T01:50:08.203141Z",
     "shell.execute_reply": "2023-11-10T01:50:08.201212Z",
     "shell.execute_reply.started": "2023-11-10T01:50:06.952131Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# count how many records we have for influencers\n",
    "df.name.value_counts().sort_values().plot(kind='barh', figsize=(8,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8051f3f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Create Small Sample DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34014e18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:09.248160Z",
     "iopub.status.busy": "2023-11-10T01:50:09.247639Z",
     "iopub.status.idle": "2023-11-10T01:50:09.275727Z",
     "shell.execute_reply": "2023-11-10T01:50:09.274577Z",
     "shell.execute_reply.started": "2023-11-10T01:50:09.248122Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df = df.sample(100)[['name', 'headline', 'content']]\n",
    "\n",
    "# drop records with missing content\n",
    "sample_df = sample_df.dropna()\n",
    "\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c476dbbd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Custom Functions\n",
    "\n",
    "Set up a custom function to help with the data transformation. \n",
    "\n",
    "```\n",
    "def custom_function(df, text_columns):\n",
    "    \n",
    "    # For text cleaning based functions use: \n",
    "    for column in text_columns: \n",
    "        df[column] = # add change code here \n",
    "    \n",
    "    #For an example of how to filter the df see number_proportion_filter.py or numeric_filer.py \n",
    "    \n",
    "    return df\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897a82e5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 🙈 Profanity Check\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f52648",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:11.258071Z",
     "iopub.status.busy": "2023-11-10T01:50:11.257532Z",
     "iopub.status.idle": "2023-11-10T01:50:11.307660Z",
     "shell.execute_reply": "2023-11-10T01:50:11.306721Z",
     "shell.execute_reply.started": "2023-11-10T01:50:11.258025Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from profanity_check import predict_prob\n",
    "\n",
    "threshold = 0.9\n",
    "\n",
    "sample_df[\"profanity\"] = predict_prob(sample_df[\"content\"])\n",
    "\n",
    "sample_df = sample_df[sample_df[\"profanity\"] < threshold]\n",
    "sample_df = sample_df.reset_index(drop=True)\n",
    "sample_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653368f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:11.945721Z",
     "iopub.status.busy": "2023-11-10T01:50:11.944630Z",
     "iopub.status.idle": "2023-11-10T01:50:12.298906Z",
     "shell.execute_reply": "2023-11-10T01:50:12.297561Z",
     "shell.execute_reply.started": "2023-11-10T01:50:11.945672Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df['profanity'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e7e842",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# 🧐 Text Quality Check\n",
    "\n",
    "## Flesch Grade reading level\n",
    "\n",
    "Depending on the task at hand, sometimes it's advantages to evaluate text and\n",
    "language based on it's estimated reading level. One popular way to do this \n",
    "is with the Flesch-Kincaid grade level.  This is a grade formula in that a score\n",
    "of 9.3 means that a ninth grader would be able to read the document.\n",
    "\n",
    "The wikipedia article on this technique does a fair job explaining it:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b1374f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Using the `textstat` python linvrary, we can approximate the reading level and\n",
    "set our thresholds for what level we find ideal for our downstream NLP task.\n",
    "\n",
    "In this case, we will limit the reading level between `2` and `10`, but many\n",
    "other ranges are certainly possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2587d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:14.284426Z",
     "iopub.status.busy": "2023-11-10T01:50:14.283577Z",
     "iopub.status.idle": "2023-11-10T01:50:14.562179Z",
     "shell.execute_reply": "2023-11-10T01:50:14.560781Z",
     "shell.execute_reply.started": "2023-11-10T01:50:14.284382Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import textstat \n",
    "\n",
    "min_grade_level = 2\n",
    "max_grade_level = 10\n",
    "\n",
    "def check_quality(df, text_column):\n",
    "    df[\"flesch_grade\"] = df[text_column].apply(textstat.flesch_kincaid_grade)\n",
    "    return df\n",
    "\n",
    "cleaned = check_quality(sample_df, 'content')\n",
    "\n",
    "# Filter to only those records with content between 2 and 10 grade level\n",
    "cleaned = cleaned[(cleaned[\"flesch_grade\"] >= min_grade_level) & (cleaned[\"flesch_grade\"] <= max_grade_level)]\n",
    "cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279d4b96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:15.487127Z",
     "iopub.status.busy": "2023-11-10T01:50:15.486591Z",
     "iopub.status.idle": "2023-11-10T01:50:15.816502Z",
     "shell.execute_reply": "2023-11-10T01:50:15.815114Z",
     "shell.execute_reply.started": "2023-11-10T01:50:15.487088Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df['flesch_grade'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dc3a21",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Run Preparation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebec03f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:17.353282Z",
     "iopub.status.busy": "2023-11-10T01:50:17.352386Z",
     "iopub.status.idle": "2023-11-10T01:50:32.076379Z",
     "shell.execute_reply": "2023-11-10T01:50:32.075075Z",
     "shell.execute_reply.started": "2023-11-10T01:50:17.353237Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic ETL\n",
    "\n",
    "# Read in some additional utility functions\n",
    "\n",
    "def remove_trailing_ws(df, text_columns):\n",
    "    # For text cleaning based functions use: \n",
    "    for column in text_columns: \n",
    "        df[column] = df[column].str.strip()\n",
    "    return df\n",
    "\n",
    "def length_check_func(df,text_column, minLength,maxLength):\n",
    "    df = df[(df[text_column].str.len() > int(minLength))]\n",
    "    df = df[(df[text_column].str.len() < int(maxLength))]\n",
    "    return df\n",
    "\n",
    "def string_replace(df, text_columns, input, output):\n",
    "    # For text cleaning based functions use: \n",
    "    for column in text_columns: \n",
    "        df[column] = df[column].str.replace(input, output)\n",
    "    return df\n",
    "\n",
    "# Keep Relevant Columns\n",
    "cleaned = df[['name', 'headline', 'about', 'content', 'reactions']]\n",
    "\n",
    "# Dropping missing\n",
    "cleaned = cleaned.dropna()\n",
    "\n",
    "\n",
    "# Text Cleaning (Minimal)\n",
    "cleaned = string_replace(cleaned, ['content'], '…see more', '')\n",
    "cleaned = remove_trailing_ws(cleaned, ['content'])\n",
    "\n",
    "max_len = 10000\n",
    "min_len = 1\n",
    "\n",
    "cleaned = length_check_func(cleaned, 'content', min_len, max_len)\n",
    "\n",
    "\n",
    "# Set thresholds\n",
    "\n",
    "# Profanity Threshold\n",
    "profanity_threshold = 0.9\n",
    "\n",
    "# Target Reading Level\n",
    "min_grade_level = 2\n",
    "max_grade_level = 10\n",
    "\n",
    "# Predict Probability of Profane Language\n",
    "cleaned[\"profanity\"] = predict_prob(cleaned[\"content\"])\n",
    "\n",
    "# Filter out Profane Language\n",
    "cleaned = cleaned[cleaned[\"profanity\"] < profanity_threshold]\n",
    "cleaned = cleaned.reset_index(drop=True)\n",
    "\n",
    "# Determine Reading Level\n",
    "cleaned = check_quality(cleaned, 'content')\n",
    "\n",
    "# Filter to only those records with content between a 2nd and 10th grader\n",
    "cleaned = cleaned[(cleaned[\"flesch_grade\"] >= min_grade_level) & (cleaned[\"flesch_grade\"] <= max_grade_level)]\n",
    "cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dedfa86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:32.079081Z",
     "iopub.status.busy": "2023-11-10T01:50:32.078554Z",
     "iopub.status.idle": "2023-11-10T01:50:32.085937Z",
     "shell.execute_reply": "2023-11-10T01:50:32.084738Z",
     "shell.execute_reply.started": "2023-11-10T01:50:32.079048Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_dropped = df.shape[0] - cleaned.shape[0]\n",
    "\n",
    "print(f\"Lost a total of {num_dropped} records, or about {round(num_dropped/df.shape[0], 2) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82311a97",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# One Step Further\n",
    "\n",
    "If we really want to talk like an influencer, perhaps we should also additionally limit the number of records\n",
    "by the number of reactions the posts got. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb86e51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:34.300043Z",
     "iopub.status.busy": "2023-11-10T01:50:34.299545Z",
     "iopub.status.idle": "2023-11-10T01:50:34.326470Z",
     "shell.execute_reply": "2023-11-10T01:50:34.325042Z",
     "shell.execute_reply.started": "2023-11-10T01:50:34.300006Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned.sort_values('reactions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205e140d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We can see that the number of reactions these posts from 'influencers' received appear to go \n",
    "from zero (yikes!) all the way to over 330k. Noticeably, that single post with ~330k reactions\n",
    "is simply `Helen is my kinda lady`.\n",
    "\n",
    "The fact this post received so much attention given it's lack of context is also \n",
    "a warning that there could be exogenous latent variables, such as current events, pop culture, etc, that could be driving \n",
    "the number of reactions, not necessarily the content itself. \n",
    "\n",
    "Let's be just a little more analytical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd069513",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:39.650715Z",
     "iopub.status.busy": "2023-11-10T01:50:39.649090Z",
     "iopub.status.idle": "2023-11-10T01:50:39.661174Z",
     "shell.execute_reply": "2023-11-10T01:50:39.659715Z",
     "shell.execute_reply.started": "2023-11-10T01:50:39.650652Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify the 90th percentile of reactions over all posts\n",
    "p90 = np.quantile(cleaned.reactions, 0.9)\n",
    "\n",
    "print(f'The 90th percentile is {p90} reactions.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcfdc74",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "For our purposes, let's focus on the top \"performing\" content to fine tune our model on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11752a02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:50:41.078148Z",
     "iopub.status.busy": "2023-11-10T01:50:41.077488Z",
     "iopub.status.idle": "2023-11-10T01:50:41.090022Z",
     "shell.execute_reply": "2023-11-10T01:50:41.088789Z",
     "shell.execute_reply.started": "2023-11-10T01:50:41.078098Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned = cleaned[cleaned.reactions > p90]\n",
    "cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f711c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We're now left with 2,127 high-quality data points to experiment with \n",
    "fine tuning on. \n",
    "\n",
    "Finally, let's ask h2oGPT to provide a title for our LinkedIn Influencer content.\n",
    "This process is called `zero-shot text generation` (more on this later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b8475",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:51:12.369670Z",
     "iopub.status.busy": "2023-11-10T01:51:12.369182Z",
     "iopub.status.idle": "2023-11-10T01:51:21.610939Z",
     "shell.execute_reply": "2023-11-10T01:51:21.609347Z",
     "shell.execute_reply.started": "2023-11-10T01:51:12.369634Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df = cleaned.sample(5)\n",
    "\n",
    "from gradio_client import Client\n",
    "import ast\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "HOST_URL = \"https://gpt-genai.h2o.ai/\"\n",
    "GPT_KEY = \"f74f043e-45fc-4dfe-9c33-55a4720427f6\"\n",
    "    \n",
    "client = Client(HOST_URL)\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "def generate_title(content):\n",
    "    \n",
    "    #try:\n",
    "    summarize_prompt = 'You are a helpful, respectful and honest assistant the specializes in generating accurate titles of LinkedIn posts. Provide a title for the following post. The title should be a single sentence, not using bullet points. Only include the title in the response. The LinkedIn post is: ' + content\n",
    "    kwargs = dict(\n",
    "        instruction_nochat=summarize_prompt, \n",
    "        h2ogpt_key = GPT_KEY)\n",
    "    \n",
    "    response = client.predict(str(dict(kwargs)), api_name='/submit_nochat_api')\n",
    "    reply = ast.literal_eval(response)['response']\n",
    "    #except:\n",
    "        #reply = 'NA'\n",
    "    return reply\n",
    "        \n",
    "sample_df['title'] = sample_df.progress_apply(lambda row :   generate_title(row['content']), axis=1)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8040bb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:51:22.779132Z",
     "iopub.status.busy": "2023-11-10T01:51:22.778601Z",
     "iopub.status.idle": "2023-11-10T01:51:22.788148Z",
     "shell.execute_reply": "2023-11-10T01:51:22.786648Z",
     "shell.execute_reply.started": "2023-11-10T01:51:22.779093Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pprint(f'CONTENT:')\n",
    "sample_df['content'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001acb4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:51:23.762650Z",
     "iopub.status.busy": "2023-11-10T01:51:23.761810Z",
     "iopub.status.idle": "2023-11-10T01:51:23.770735Z",
     "shell.execute_reply": "2023-11-10T01:51:23.769707Z",
     "shell.execute_reply.started": "2023-11-10T01:51:23.762609Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'TITLE:')\n",
    "sample_df['title'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf5c7ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T01:51:24.548964Z",
     "iopub.status.busy": "2023-11-10T01:51:24.547836Z",
     "iopub.status.idle": "2023-11-10T01:51:24.574766Z",
     "shell.execute_reply": "2023-11-10T01:51:24.573266Z",
     "shell.execute_reply.started": "2023-11-10T01:51:24.548917Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional alternative for fine tuning - create an instruction.\n",
    "cleaned['instruction'] = ('Write a LinkedIn post in the style of an influencer whom has the title of '\n",
    "  + cleaned['headline'] + ' and can be described by the following: ' \n",
    "  + cleaned['about'])\n",
    "\n",
    "cleaned.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae17bc7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Output Dataset\n",
    "\n",
    "Now we're ready to store our data set out and experiment with fine-tuning\n",
    "in Lab # 2 with H2O LLM Studio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b25ad",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-11-10T01:52:01.078715Z",
     "iopub.status.busy": "2023-11-10T01:52:01.078185Z",
     "iopub.status.idle": "2023-11-10T03:01:13.098104Z",
     "shell.execute_reply": "2023-11-10T03:01:13.096087Z",
     "shell.execute_reply.started": "2023-11-10T01:52:01.078668Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "# WARNING! This could take a very long time using the public facing h2oGPT endpoint. ##\n",
    "#######################################################################################\n",
    "\n",
    "# Apply to full data set\n",
    "cleaned['title'] = cleaned.progress_apply(lambda row :   generate_title(row['content']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c630f9f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-10T03:12:13.080002Z",
     "iopub.status.busy": "2023-11-10T03:12:13.079498Z",
     "iopub.status.idle": "2023-11-10T03:12:13.513635Z",
     "shell.execute_reply": "2023-11-10T03:12:13.512360Z",
     "shell.execute_reply.started": "2023-11-10T03:12:13.079963Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Output locally\n",
    "cleaned.to_csv('influencers_data_prepared.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc2bdd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 38.069262,
   "end_time": "2023-11-10T03:16:58.522503",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-10T03:16:20.453241",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
